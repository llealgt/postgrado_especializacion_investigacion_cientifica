{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.15</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Warning: You are not running the latest version of PixieDust. Current is 1.1.15, Latest is 1.1.17</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>Please copy and run the following command in a new cell to upgrade: <span style=\"background-color:#ececec;font-family:monospace;padding:0 5px\">!pip install --user --upgrade pixiedust</span></div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Please restart kernel after upgrading.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pixiedust\n",
    "from torchvision import transforms,datasets,models\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas y recordatorios\n",
    "\n",
    "* Por la forma en que se prepararon los datos renombre  temporalmente el paciente 120 a paciente 6(para mantener la continuidad de la muestra) y lo puse también como 6 en los diagnosticos.\n",
    "* Similar al punto anterior pero con el paciente 121 y el 7\n",
    "* Similar pero con el paciente 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fast experimentation on slow computer limit the size of the sample\n",
    "MAX_PATIENTS = 250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../datos/TOMOGRAFIAS CHEQUEADAS 1 - 400/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosticos  = pd.read_excel(DATA_DIRECTORY+\"RESUMEN TAC CEREBRALES.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paciente</th>\n",
       "      <th>hemorragia</th>\n",
       "      <th>isquemia</th>\n",
       "      <th>fractura</th>\n",
       "      <th>masa</th>\n",
       "      <th>edema</th>\n",
       "      <th>observaciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>torax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urotac revisar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no están las imágenes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paciente  hemorragia  isquemia  fractura  masa  edema  \\\n",
       "0          1         1.0       0.0       0.0   0.0    0.0   \n",
       "1          2         1.0       0.0       1.0   0.0    1.0   \n",
       "2          3         1.0       0.0       1.0   0.0    0.0   \n",
       "3          4         0.0       1.0       0.0   0.0    0.0   \n",
       "4          5         0.0       0.0       0.0   1.0    1.0   \n",
       "5          6         1.0       0.0       0.0   0.0    0.0   \n",
       "6          7         0.0       0.0       0.0   0.0    0.0   \n",
       "7          8         0.0       0.0       0.0   0.0    0.0   \n",
       "8          9         0.0       0.0       0.0   0.0    0.0   \n",
       "9         10         0.0       0.0       0.0   0.0    0.0   \n",
       "10        11         0.0       0.0       0.0   0.0    0.0   \n",
       "11        12         0.0       0.0       0.0   0.0    0.0   \n",
       "12        13         NaN       NaN       NaN   NaN    NaN   \n",
       "13        14         NaN       NaN       NaN   NaN    NaN   \n",
       "14        15         1.0       0.0       1.0   0.0    0.0   \n",
       "15        16         NaN       NaN       NaN   NaN    NaN   \n",
       "16        17         1.0       0.0       1.0   0.0    1.0   \n",
       "17        18         0.0       0.0       0.0   0.0    0.0   \n",
       "18        19         1.0       0.0       0.0   0.0    0.0   \n",
       "19        20         0.0       0.0       0.0   0.0    0.0   \n",
       "\n",
       "            observaciones  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "5                     NaN  \n",
       "6                     NaN  \n",
       "7                     NaN  \n",
       "8                     NaN  \n",
       "9                     NaN  \n",
       "10                    NaN  \n",
       "11                    NaN  \n",
       "12                  torax  \n",
       "13         urotac revisar  \n",
       "14                    NaN  \n",
       "15                abdomen  \n",
       "16                    NaN  \n",
       "17                    NaN  \n",
       "18  no están las imágenes  \n",
       "19                    NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosticos.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diccionario cuya llave es el id de paciente y el valor una lista \n",
    "# donde cada elemento de la lista es la matriz de una i\n",
    "diccionario_imagenes_pacientes = dict()\n",
    "processed_patients = 0\n",
    "\n",
    "for paciente in diagnosticos.paciente:\n",
    "    if processed_patients >= MAX_PATIENTS:\n",
    "        diagnosticos = diagnosticos.iloc[:processed_patients]\n",
    "        break\n",
    "    directorio_paciente = DATA_DIRECTORY+\"paciente_\"+str(paciente)\n",
    "    \n",
    "    # if patient directory is missing OR any of the diagnostics is null \n",
    "    #do not try to read images and delete it from diagnostics dataframe\n",
    "    if not os.path.exists(directorio_paciente) or  diagnosticos[diagnosticos.paciente == paciente].iloc[:,0:6].isnull().values.any():\n",
    "        diagnostics_row = diagnosticos[diagnosticos.paciente == paciente]\n",
    "        diagnosticos.drop(int(diagnostics_row.index.values),axis=0,inplace=True)\n",
    "        \n",
    "        continue\n",
    "    archivos_paciente = os.listdir(directorio_paciente)\n",
    "    \n",
    "    lista_imagenes_paciente = []\n",
    "    for archivo in archivos_paciente:\n",
    "        if archivo.endswith(\".jpg\"):\n",
    "            imagen = mpimg.imread(directorio_paciente+\"/\"+archivo)\n",
    "            lista_imagenes_paciente.append(imagen)\n",
    "            \n",
    "    processed_patients += 1\n",
    "            \n",
    "    diccionario_imagenes_pacientes[paciente] = lista_imagenes_paciente\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paciente</th>\n",
       "      <th>hemorragia</th>\n",
       "      <th>isquemia</th>\n",
       "      <th>fractura</th>\n",
       "      <th>masa</th>\n",
       "      <th>edema</th>\n",
       "      <th>observaciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paciente  hemorragia  isquemia  fractura  masa  edema observaciones\n",
       "0          1         1.0       0.0       0.0   0.0    0.0           NaN\n",
       "1          2         1.0       0.0       1.0   0.0    1.0           NaN\n",
       "2          3         1.0       0.0       1.0   0.0    0.0           NaN\n",
       "3          4         0.0       1.0       0.0   0.0    0.0           NaN\n",
       "4          5         0.0       0.0       0.0   1.0    1.0           NaN\n",
       "5          6         1.0       0.0       0.0   0.0    0.0           NaN\n",
       "6          7         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "7          8         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "8          9         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "9         10         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "10        11         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "11        12         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "14        15         1.0       0.0       1.0   0.0    0.0           NaN\n",
       "16        17         1.0       0.0       1.0   0.0    1.0           NaN\n",
       "17        18         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "19        20         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "20        21         1.0       0.0       0.0   0.0    0.0           NaN\n",
       "21        22         1.0       0.0       1.0   0.0    1.0           NaN\n",
       "22        23         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "23        24         0.0       0.0       0.0   0.0    0.0           NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosticos.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos y arquitecturas\n",
    "### Arquitecturas experimental  DNC\n",
    "* Alimentamos al modelo imagen por imagen y se presenta un solo diagnostico por paciente\n",
    "* El controller de la DNC esta compuesto por una convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_OUTPUT_SIZE = 128\n",
    "READ_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: cambiar valores quemados por valores parametrizados y calculos dependientes\n",
    "class ConvController(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,4,kernel_size=3,stride=1)\n",
    "        self.fc1  =  torch.nn.Linear(262144,CONTROLLER_OUTPUT_SIZE)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        h = self.conv1(x)\n",
    "        \n",
    "        #flatten\n",
    "        h =  x.view(-1,x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        h =  self.fc1(h)\n",
    "        \n",
    "        return h #h_t in my txt\n",
    "    \n",
    "class Controller(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_controller = ConvController()\n",
    "        self.fc1 = torch.nn.Linear(10,CONTROLLER_OUTPUT_SIZE)\n",
    "        self.fc2 = torch.nn.Linear(2*CONTROLLER_OUTPUT_SIZE,CONTROLLER_OUTPUT_SIZE)\n",
    "        \n",
    "    def forward(self,x,read_vectors):\n",
    "        h_conv = self.conv_controller(x)\n",
    "        h_read_vectors = self.fc1(read_vectors)\n",
    "        \n",
    "        h_t = torch.cat((h_conv,h_read_vectors),dim=1)\n",
    "        \n",
    "        h_t =  torch.relu( h_t)\n",
    "        h_t =  self.fc2(h_t) \n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: cambiar valores quemados por valores parametrizados y calculos dependientes\n",
    "#TODO: cordar por que en algun momento le puse bias = False a los pesos del vector de salida de la DNC\n",
    "\n",
    "\n",
    "class DNC(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,controller,memory_size = (10,10),read_heads = 1,device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.controller = controller\n",
    "        self.device = device\n",
    "        self.N = memory_size[0] # number of memory locations\n",
    "        self.W = memory_size[1] # word size of the memory \n",
    "        self.R = read_heads # number of read heads\n",
    "        self.WS = 1 #not in the paper(they use 1), but used as a parametrizable number of write heads for further experiments\n",
    "        self.interface_vector_size = (self.W*self.R) + (self.W*self.WS) + (2*self.W) + (5*self.R) + 3\n",
    "        \n",
    "        # inicialization st to random just for testing, remember to put on zeros\n",
    "        #self.memory_matrix = self.memory_matrix =  nn.Parameter(torch.zeros(size=memory_size),requires_grad= False) \n",
    "        \n",
    "        #1024 es el tamaño del vector de salida del controlador, 1 es el tamaño de salida de la dnc\n",
    "        self.output_vector_linear = torch.nn.Linear(CONTROLLER_OUTPUT_SIZE,1,bias=True) #W_y \n",
    "        self.interface_vector_linear = torch.nn.Linear(CONTROLLER_OUTPUT_SIZE,self.interface_vector_size,bias=True) #W_ξ\n",
    "        self.read_vectors_to_output_linear = torch.nn.Linear(self.R*self.W,1,bias = True) #W_r in my txt\n",
    "        \n",
    "        self.read_keys = torch.Tensor(size=(self.R,self.W)).requires_grad_(False) # k_r in my txt\n",
    "        self.read_strenghts = torch.Tensor(size=(self.R,1)).requires_grad_(False) #β_r\n",
    "        \n",
    "        #self.read_weighting = torch.Tensor(torch.zeros(size=(self.R,self.N))).requires_grad_(False).to(device) #r_w\n",
    "        \n",
    "        self.write_key = torch.Tensor(size=(1,self.W)).requires_grad_(False) # k_w in my txt\n",
    "        self.write_strenght = torch.Tensor(size=(1,1)).requires_grad_(False) # β_w\n",
    "        \n",
    "        #self.write_weighting = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(False) # w_w\n",
    "        \n",
    "        #self.usage_vector = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(False) #u_t\n",
    "        \n",
    "        self.memory_matrix_ones = torch.Tensor(torch.ones(size=memory_size)).requires_grad_(True).to(device) #E on paper\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def forward(self,x,read_vectors):\n",
    "        \n",
    "        h_t = self.controller(x,read_vectors) #controller output called ht in the paper\n",
    "        \n",
    "        output_vector = self.output_vector_linear(h_t) # called Vt in the paper(υ=Wy[h1;...;hL]) v_o_t in my txt\n",
    "        interface_vector = self.interface_vector_linear(h_t).data #called ξt(ksi) in the paper ,ξ_t in my txt\n",
    "        \n",
    "        self.read_keys.data = interface_vector[0,0:self.R*self.W].view((self.R,self.W)) #k_r in my txt\n",
    "        \n",
    "        #clamp temporary added because the exp was returning inf  values\n",
    "        read_strenghts =  torch.clamp( interface_vector[0,self.R*self.W:self.R*self.W+self.R].view((self.R,1)),max=85)\n",
    "        self.read_strenghts.data = self.oneplus(read_strenghts) #β_r\n",
    "        \n",
    "        self.write_key.data = interface_vector[0,self.R*self.W+self.R:self.R*self.W+self.R+self.W].view((1,self.W)) # k_w\n",
    "        \n",
    "        write_strenght = torch.clamp(interface_vector[:,self.R*self.W+self.R+self.W:self.R*self.W+self.R+self.W + 1].view((1,1)),max=85)\n",
    "        self.write_strenght.data = self.oneplus(write_strenght) #β_w\n",
    "        \n",
    "        erase_vector = interface_vector[0,self.R*self.W+self.R+self.W + 1: self.R*self.W+self.R+self.W + 1 + self.W].view((1,self.W))\n",
    "        erase_vector = torch.sigmoid(erase_vector) #e_t\n",
    "        \n",
    "        write_vector = interface_vector[0,self.R*self.W+self.R+self.W + 1 + self.W:self.R*self.W+self.R+self.W + 1 + 2*self.W].view((1,self.W)) #v_t\n",
    "        \n",
    "        free_gates  =  interface_vector[0,self.R*self.W+self.R+self.W + 1 + 2*self.W:self.R*self.W+2*self.R+self.W + 1 + 2*self.W].view((self.R,1)) #f_t\n",
    "        free_gates =   torch.sigmoid(free_gates)\n",
    "        \n",
    "        allocation_gate = interface_vector[0,self.R*self.W+2*self.R+self.W + 1 + 2*self.W:self.R*self.W+2*self.R+self.W + 1 + 2*self.W+1]\n",
    "        allocation_gate = torch.sigmoid(allocation_gate)\n",
    "        \n",
    "        write_gate = interface_vector[0,self.R*self.W+2*self.R+self.W + 1 + 2*self.W+1:self.R*self.W+2*self.R+self.W + 1 + 2*self.W+2]\n",
    "        write_gate = torch.sigmoid( write_gate)\n",
    "        \n",
    "        \n",
    "        # Escritura\n",
    "        # TODO: verificar y/o experimentar si el ordern es :primero escribir y luego leer de la memoria(asi parece en el pazper)\n",
    "        retention_vector = (1.0 - free_gates * self.read_weighting).prod(dim=0)\n",
    "        self.usage_vector.data = (self.usage_vector +self.write_weighting - (self.usage_vector *self.write_weighting))*retention_vector #u_t\n",
    "        allocation_weighting = self.calc_allocation_weighting(self.usage_vector)\n",
    "        write_content_weighting = self.content_lookup(self.memory_matrix,self.write_key,self.write_strenght)\n",
    "\n",
    "        self.write_weighting.data =  write_gate*(  \n",
    "            (allocation_gate * allocation_weighting) +  ((1- allocation_gate)*write_content_weighting))\n",
    "        \n",
    "        new_memory_matrix = self.memory_matrix*(self.memory_matrix_ones - torch.matmul(self.write_weighting.t(),erase_vector)) + torch.matmul(self.write_weighting.t(),write_vector)\n",
    "        \n",
    "        self.memory_matrix.data = new_memory_matrix\n",
    "        \n",
    "        # read by content weithing(attention by similarity)\n",
    "        read_content_weighting = self.content_lookup(self.memory_matrix,self.read_keys,self.read_strenghts)\n",
    "        \n",
    "        #read weithing is a combination of reading modes,TODO:add temporal attention not just by similarity\n",
    "        self.read_weighting.data = read_content_weighting\n",
    "        \n",
    "        read_vectors = torch.matmul(self.read_weighting,self.memory_matrix).view((1,self.R*self.W)) #r in my txt\n",
    "        read_heads_to_output = self.read_vectors_to_output_linear(read_vectors) #v_r_t in my t xt\n",
    "        \n",
    "        #TODO: experiment and decide if maintain sigmoid\n",
    "        y_t = torch.sigmoid(output_vector + read_heads_to_output)\n",
    "        return y_t,read_vectors\n",
    "    \n",
    "    def oneplus(self,x):\n",
    "        # apply oneplus operation to a tensor to constrain it's elements to [1,inf)\n",
    "        #TODO: check numerical statiliby as exp is returning inf for numbers like 710,emporary added clamp to 85\n",
    "        return torch.log(1+torch.exp(x)) + 1\n",
    "    \n",
    "    def content_lookup(self,matrix,keys,strengths):\n",
    "        # returns a probability distribution over the memory locations \n",
    "        # with higher probability to memory locations with bigger similarity to the keys\n",
    "        # bigger strenght make more aggresive distributions ,for example a distribution (0.2,0.3,0.5) with\n",
    "        # bigger strenght becomes (0.1,0.12,0.78)\n",
    "        # returns tensor of shape (read keys,memory size) = (R,N)\n",
    "        keys_norm =  torch.sqrt(torch.sum(keys**2,dim=1).unsqueeze(dim=1))\n",
    "        matrix_norm = torch.sqrt(torch.sum(matrix**2,dim=1))\n",
    "        norms_multiplication = keys_norm*matrix_norm\n",
    "        # calc cosine similarity between keys and memory locations(1e-6 is used avoiding div by 0)\n",
    "        divide_zero_prevent_factor = torch.zeros_like(norms_multiplication).add_(1e-6)\n",
    "        cosine_similarity = torch.matmul(keys,matrix.t())/(torch.max(norms_multiplication,divide_zero_prevent_factor))\n",
    "        \n",
    "        # do a \"strenght\" softmax to calculate the probability distribution\n",
    "        numerator = torch.exp(cosine_similarity*strengths)\n",
    "        denominator = numerator.sum(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "        distribution = numerator/denominator\n",
    "        \n",
    "        return distribution\n",
    "    \n",
    "    def calc_allocation_weighting(self,usage_vector):\n",
    "        #print(\"usage vector\",usage_vector)\n",
    "        _,free_list = torch.topk(-usage_vector,self.N,dim=1) #φt indices of memory locations ordered by usage\n",
    "        #print(\"free list\",free_list)\n",
    "        free_list = free_list.view(-1)\n",
    "        #print(\"reshaped free list\",free_list)\n",
    "        _,ordered_free_list =  torch.topk(-free_list,self.N)\n",
    "        ordered_free_list = ordered_free_list.view(-1)\n",
    "        #print(\"ordered free list\",ordered_free_list)\n",
    "        ordered_usage_vector = usage_vector[:,free_list]\n",
    "        #print(\"ordered usage vector\",ordered_usage_vector)\n",
    "        ordered_usage_vector_cumulative_product = torch.ones(size=(1,self.N+1)).to(device)\n",
    "        #print(ordered_usage_vector_cumulative_product)\n",
    "        #print(\"cumprod \",ordered_usage_vector.cumprod(dim=1))\n",
    "        ordered_usage_vector_cumulative_product[0,1:] = ordered_usage_vector.cumprod(dim=1)\n",
    "        #print(ordered_usage_vector_cumulative_product)\n",
    "        \n",
    "        allocation_weighting = (1 - usage_vector)*ordered_usage_vector_cumulative_product[0,ordered_free_list]\n",
    "        \n",
    "        return  allocation_weighting\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory_matrix =  torch.Tensor(torch.zeros(size=(self.N,self.W))).requires_grad_(True).to(device) \n",
    "        self.read_weighting = torch.Tensor(torch.zeros(size=(self.R,self.N))).requires_grad_(True).to(device) #r_w\n",
    "        self.write_weighting = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(True).to(device) # w_w\n",
    "        self.usage_vector = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(True).to(device) #u_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos\n",
    "* Experimentando con DNC alimentando una imagen a la vez en orden aleatorio con pacientes también en orden aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_controller = Controller()\n",
    "dnc_model = DNC(controller=conv_controller,memory_size = (5,5),read_heads=2,device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_criterion = torch.nn.BCELoss()\n",
    "def loss_function(y,y_hat,last_flag):\n",
    "    #print(y,y_hat,last_flag)\n",
    "    #base_criterion = torch.nn.BCELoss()\n",
    "    return torch.full_like(y,last_flag) * base_criterion(y,y_hat)\n",
    "    #return base_criterion(y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = loss_function\n",
    "optimizer = optim.Adam(dnc_model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "source": [
    "\n",
    "total_accuracies  = []\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_predictions = []\n",
    "    epoch_real_values = []\n",
    "    # en cada epoch procesar los pacientes en orden aleatorio\n",
    "    pacientes = np.random.choice(np.array(diagnosticos.paciente),size= len(diagnosticos.paciente),replace=False)\n",
    "    \n",
    "    conteo_pacientes = 0\n",
    "    for paciente in pacientes:\n",
    "        #TODO: remover esta validacion, solo puesta para probar una unica iteracion en compu lenta\n",
    "        if conteo_pacientes >= 99999999:\n",
    "            break\n",
    "            \n",
    "        dnc_model.reset()\n",
    "        read_vectors = torch.zeros(size=(1,dnc_model.R*dnc_model.W)).to(device)\n",
    "        \n",
    "        imagenes_paciente = diccionario_imagenes_pacientes.get(paciente)\n",
    "        diagnostico_hemorragia_paciente = np.array(float(diagnosticos[diagnosticos.paciente==paciente].hemorragia))\n",
    "        tensor_diagnostico_hemorragia_paciente = torch.Tensor(diagnostico_hemorragia_paciente).to(device)\n",
    "        \n",
    "        indices_imagenes_pacientes = np.arange(0,len(imagenes_paciente)-1,step=1)\n",
    "        indices_aleatorios_imagenes = np.random.choice(indices_imagenes_pacientes,len(indices_imagenes_pacientes),replace=False)\n",
    "        \n",
    "        losses = []\n",
    "        for indice in indices_aleatorios_imagenes:\n",
    "            last_image =  int(indice  == indices_aleatorios_imagenes[-1])\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            \n",
    "            imagen_paciente = imagenes_paciente[indice]\n",
    "            \n",
    "            if imagen_paciente.shape != (512,512):\n",
    "                #TODO: tread different image sizes with reshaping, resizing(or other ideas)\n",
    "                continue\n",
    "                \n",
    "            tensor_imagen_paciente =  torch.unsqueeze(\n",
    "                torch.unsqueeze( torch.Tensor(imagen_paciente),dim=0),dim=1).to(device)\n",
    "            \n",
    "            #print(\"Alimentando paciente {} e imagen {} al modelo\".format(paciente,indice),imagen_paciente.shape)\n",
    "            \n",
    "            diagnostico_hemorragia_aproximado,read_vectors = dnc_model(tensor_imagen_paciente,read_vectors)\n",
    "            loss = criterion(diagnostico_hemorragia_aproximado,tensor_diagnostico_hemorragia_paciente,last_image)\n",
    "            \n",
    "            losses.append(loss.view((1,1)))\n",
    "            \n",
    "            if last_image:\n",
    "                y_hat = diagnostico_hemorragia_aproximado.data.cpu().numpy()[0][0]\n",
    "                y_hat_hard = float(y_hat >= 0.5)\n",
    "                epoch_predictions.append(y_hat_hard)\n",
    "                epoch_real_values.append(float(diagnostico_hemorragia_paciente))\n",
    "                \n",
    "                #print(\"--Flag ultima imagen:{} diagnostico:{} valor real{}\".format(last_image,y_hat,diagnostico_hemorragia_paciente))\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                patient_loss = torch.cat(losses).sum()\n",
    "                \n",
    "                patient_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "        conteo_pacientes += 1\n",
    "            \n",
    "    epoch_predictions = np.array(epoch_predictions)\n",
    "    epoch_real_values = np.array(epoch_real_values)\n",
    "    correct_predictions = epoch_predictions == epoch_real_values\n",
    "    accuracy = np.average(correct_predictions)\n",
    "    total_accuracies.append(accuracy)\n",
    "    print(\"Epoch {}: accuracy {}\".format(epoch,accuracy),epoch_predictions,epoch_real_values)\n",
    "\n",
    "print(np.average(total_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controller.conv_controller.conv1.weight\n",
      "controller.conv_controller.conv1.bias\n",
      "controller.conv_controller.fc1.weight\n",
      "controller.conv_controller.fc1.bias\n",
      "controller.fc1.weight\n",
      "controller.fc1.bias\n",
      "controller.fc2.weight\n",
      "controller.fc2.bias\n",
      "output_vector_linear.weight\n",
      "output_vector_linear.bias\n",
      "interface_vector_linear.weight\n",
      "interface_vector_linear.bias\n",
      "read_vectors_to_output_linear.weight\n",
      "read_vectors_to_output_linear.bias\n"
     ]
    }
   ],
   "source": [
    "#TODO: averiguar por que salen 6 tensores de parametros si solo se han declarado 3(al momento de correr lap rueba)\n",
    "train_parmams = list(dnc_model.named_parameters())\n",
    "\n",
    "for train_param in train_parmams:\n",
    "    print(train_param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnc_model.memory_matrix.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta (por detallar))\n",
    "* L temporal link matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM con conv\n",
    "* Experimentando con lstm alimentando una imagen a la vez en orden aleatorio con pacientes también en orden aleatorio\n",
    "\n",
    "El vector de entrada de la lstm es un vector producido por una convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVNET_OUTPUT_SIZE = 1024\n",
    "CONVNET_HIDDEN_SIZE = 1024\n",
    "\n",
    "LSTM_HIDDEN_SIZE = 1024\n",
    "\n",
    "FINAL_LAYER_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/pytorch_challenge/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "architecture = 'densenet121'\n",
    "architecture_constructor = getattr(models,architecture)\n",
    "model  =  architecture_constructor(pretrained=True)\n",
    "features_size = list(model.children())[-1].in_features\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#freeze parameters so we don't backpropagete  through them\n",
    "layers_to_freeze = 0\n",
    "layer_num = 0\n",
    "for parameter in model.parameters():\n",
    "    if layer_num >= layers_to_freeze:\n",
    "        break\n",
    "    parameter.requires_grad = False\n",
    "        \n",
    "    layer_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier = torch.nn.Sequential(OrderedDict([\n",
    "    (\"fc1\",torch.nn.Linear(features_size,CONVNET_OUTPUT_SIZE)),\n",
    "    #(\"relu\",torch.nn.ReLU()),\n",
    "    #(\"fc2\",torch.nn.Linear(CONVNET_HIDDEN_SIZE,CONVNET_HIDDEN_SIZE)),\n",
    "    #(\"relu2\",torch.nn.ReLU()),\n",
    "    #(\"fc3\",torch.nn.Linear(CONVNET_HIDDEN_SIZE,CONVNET_OUTPUT_SIZE))\n",
    "]))\n",
    "\n",
    "model.classifier = model_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,conv_net,lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_net = conv_net\n",
    "        self.lstm = nn.LSTM(input_size= CONVNET_OUTPUT_SIZE,hidden_size = LSTM_HIDDEN_SIZE,num_layers=lstm_layers,batch_first = True)\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_hidden_size = LSTM_HIDDEN_SIZE\n",
    "        \n",
    "        self.output_linear = nn.Linear(LSTM_HIDDEN_SIZE,3)\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        x,hidden = self.lstm(x,hidden)\n",
    "        x = x.contiguous().view(-1,self.lstm_hidden_size)\n",
    "        \n",
    "        x = self.output_linear(x)\n",
    "        #x = torch.sigmoid(self.output_linear(x))\n",
    "        \n",
    "        return x,hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weigths =  next(self.lstm.parameters())\n",
    "        \n",
    "        \n",
    "        hidden = ( \n",
    "            weigths.new(self.lstm_layers,1,LSTM_HIDDEN_SIZE).zero_().to(device)\n",
    "        ,   weigths.new(self.lstm_layers,1,LSTM_HIDDEN_SIZE).zero_().to(device)\n",
    "                 )\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ya que estamos usando densenet121 transformamos los datos de entrada para que tengan el tamaño adecuado\n",
    "# y se normalicen usando los valores de media y desviación estandar del dataset usado en densenet\n",
    "train_data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lstm = ConvLSTM(model,lstm_layers=3)\n",
    "conv_lstm.to(device)\n",
    "\n",
    "base_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_pred,y_real):\n",
    "    \"for calculating the accurracy of multiple columns\"\n",
    "    assert y_pred.shape[1] == y_real.shape[1]\n",
    "    \n",
    "    num_columns = y_pred.shape[1]\n",
    "    \n",
    "    accuracies = []\n",
    "    for i in range(num_columns):\n",
    "        \n",
    "        colum_acc = accuracy_score(y_pred[:,i],y_real[:,i])\n",
    "        accuracies.append(colum_acc)\n",
    "        \n",
    "    return accuracies, np.mean(np.array(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: individual accs:[[0.6753246753246753, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8181818181818182 loss:0.6133951544761658\n",
      "Epoch 1: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5835570096969604\n",
      "Epoch 2: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5760027170181274\n",
      "Epoch 3: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5719767212867737\n",
      "Epoch 4: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5693480372428894\n",
      "Epoch 5: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5676479339599609\n",
      "Epoch 6: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5672230124473572\n",
      "Epoch 7: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5673640966415405\n",
      "Epoch 8: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5668346881866455\n",
      "Epoch 9: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5661498308181763\n",
      "Epoch 10: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5657147765159607\n",
      "Epoch 11: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5654608011245728\n",
      "Epoch 12: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5650544166564941\n",
      "Epoch 13: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5647258162498474\n",
      "Epoch 14: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5647971034049988\n",
      "Epoch 15: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5645955204963684\n",
      "Epoch 16: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5644248127937317\n",
      "Epoch 17: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5641237497329712\n",
      "Epoch 18: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5641341209411621\n",
      "Epoch 19: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5642467141151428\n",
      "Epoch 20: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5640209913253784\n",
      "Epoch 21: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5638036727905273\n",
      "Epoch 22: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5634368062019348\n",
      "Epoch 23: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5633376240730286\n",
      "Epoch 24: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5634285807609558\n",
      "Epoch 25: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.563167154788971\n",
      "Epoch 26: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5632122159004211\n",
      "Epoch 27: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5631882548332214\n",
      "Epoch 28: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5630186796188354\n",
      "Epoch 29: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5633847713470459\n",
      "Epoch 30: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5635496973991394\n",
      "Epoch 31: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5635212659835815\n",
      "Epoch 32: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5634737014770508\n",
      "Epoch 33: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5636582374572754\n",
      "Epoch 34: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5634710192680359\n",
      "Epoch 35: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5633721351623535\n",
      "Epoch 36: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5641279220581055\n",
      "Epoch 37: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5632874965667725\n",
      "Epoch 38: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5641160011291504\n",
      "Epoch 39: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5641733407974243\n",
      "Epoch 40: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5638093948364258\n",
      "Epoch 41: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5640226602554321\n",
      "Epoch 42: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5645917057991028\n",
      "Epoch 43: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5627800822257996\n",
      "Epoch 44: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5626395344734192\n",
      "Epoch 45: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5627961158752441\n",
      "Epoch 46: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5634668469429016\n",
      "Epoch 47: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5628799796104431\n",
      "Epoch 48: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5606962442398071\n",
      "Epoch 49: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5599170923233032\n",
      "Epoch 50: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5598998069763184\n",
      "Epoch 51: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5592993497848511\n",
      "Epoch 52: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5583043098449707\n",
      "Epoch 53: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.558045506477356\n",
      "Epoch 54: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5580595135688782\n",
      "Epoch 55: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5573691725730896\n",
      "Epoch 56: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5568398237228394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: individual accs:[[0.683982683982684, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8210678210678211 loss:0.5567858219146729\n",
      "Epoch 58: individual accs:[[0.7359307359307359, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8383838383838383 loss:0.5549907088279724\n",
      "Epoch 59: individual accs:[[0.7619047619047619, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8470418470418469 loss:0.5550366640090942\n",
      "Epoch 60: individual accs:[[0.7965367965367965, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8585858585858586 loss:0.554269015789032\n",
      "Epoch 61: individual accs:[[0.7532467532467533, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8441558441558442 loss:0.5546345114707947\n",
      "Epoch 62: individual accs:[[0.8008658008658008, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.86002886002886 loss:0.5529546141624451\n",
      "Epoch 63: individual accs:[[0.8008658008658008, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.86002886002886 loss:0.5530650615692139\n",
      "Epoch 64: individual accs:[[0.7878787878787878, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8556998556998557 loss:0.5548505187034607\n",
      "Epoch 65: individual accs:[[0.8008658008658008, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.86002886002886 loss:0.5547096133232117\n",
      "Epoch 66: individual accs:[[0.8311688311688312, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8701298701298702 loss:0.5521225929260254\n",
      "Epoch 67: individual accs:[[0.8441558441558441, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8744588744588745 loss:0.5516161322593689\n",
      "Epoch 68: individual accs:[[0.8874458874458875, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.888888888888889 loss:0.5499732494354248\n",
      "Epoch 69: individual accs:[[0.8831168831168831, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8874458874458875 loss:0.549992561340332\n",
      "Epoch 70: individual accs:[[0.9177489177489178, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8989898989898991 loss:0.5486603379249573\n",
      "Epoch 71: individual accs:[[0.9177489177489178, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.8989898989898991 loss:0.5487365126609802\n",
      "Epoch 72: individual accs:[[0.922077922077922, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9004329004329005 loss:0.5488214492797852\n",
      "Epoch 73: individual accs:[[0.9437229437229437, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9076479076479077 loss:0.547405481338501\n",
      "Epoch 74: individual accs:[[0.9523809523809523, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9105339105339105 loss:0.5472192168235779\n",
      "Epoch 75: individual accs:[[0.9393939393939394, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9062049062049061 loss:0.5465379953384399\n",
      "Epoch 76: individual accs:[[0.948051948051948, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9090909090909092 loss:0.5465436577796936\n",
      "Epoch 77: individual accs:[[0.9307359307359307, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9033189033189033 loss:0.5467811822891235\n",
      "Epoch 78: individual accs:[[0.9393939393939394, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9062049062049061 loss:0.5469465851783752\n",
      "Epoch 79: individual accs:[[0.961038961038961, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9134199134199134 loss:0.546462893486023\n",
      "Epoch 80: individual accs:[[0.961038961038961, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9134199134199134 loss:0.545266330242157\n",
      "Epoch 81: individual accs:[[0.9567099567099567, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.911976911976912 loss:0.5469120144844055\n",
      "Epoch 82: individual accs:[[0.948051948051948, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9090909090909092 loss:0.5465551018714905\n",
      "Epoch 83: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5449362993240356\n",
      "Epoch 84: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5452035069465637\n",
      "Epoch 85: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5448050498962402\n",
      "Epoch 86: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.5450688004493713\n",
      "Epoch 87: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.5451872944831848\n",
      "Epoch 88: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5455823540687561\n",
      "Epoch 89: individual accs:[[0.974025974025974, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9177489177489178 loss:0.5447055101394653\n",
      "Epoch 90: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.545019805431366\n",
      "Epoch 91: individual accs:[[0.9696969696969697, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9163059163059163 loss:0.5465601086616516\n",
      "Epoch 92: individual accs:[[0.9696969696969697, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9163059163059163 loss:0.5445570349693298\n",
      "Epoch 93: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5438414216041565\n",
      "Epoch 94: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.5436744093894958\n",
      "Epoch 95: individual accs:[[0.9696969696969697, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9163059163059163 loss:0.543796181678772\n",
      "Epoch 96: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.5440732836723328\n",
      "Epoch 97: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.5435830950737\n",
      "Epoch 98: individual accs:[[0.974025974025974, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9177489177489178 loss:0.5440598726272583\n",
      "Epoch 99: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5421479940414429\n",
      "Epoch 100: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5421748757362366\n",
      "Epoch 101: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.5427025556564331\n",
      "Epoch 102: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.5417292714118958\n",
      "Epoch 103: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.542110025882721\n",
      "Epoch 104: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.54282546043396\n",
      "Epoch 105: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.5427302718162537\n",
      "Epoch 106: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.5416110157966614\n",
      "Epoch 107: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5411423444747925\n",
      "Epoch 108: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.5415760278701782\n",
      "Epoch 109: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.5414594411849976\n",
      "Epoch 110: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5417072772979736\n",
      "Epoch 111: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.541175365447998\n",
      "Epoch 112: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.541027843952179\n",
      "Epoch 113: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5414661169052124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5417712926864624\n",
      "Epoch 115: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5406211018562317\n",
      "Epoch 116: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.540932834148407\n",
      "Epoch 117: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.541297197341919\n",
      "Epoch 118: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5412563681602478\n",
      "Epoch 119: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5405207872390747\n",
      "Epoch 120: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5404410362243652\n",
      "Epoch 121: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5400531888008118\n",
      "Epoch 122: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5402291417121887\n",
      "Epoch 123: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5402691960334778\n",
      "Epoch 124: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.5407341122627258\n",
      "Epoch 125: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.5421977043151855\n",
      "Epoch 126: individual accs:[[0.987012987012987, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9220779220779222 loss:0.540813148021698\n",
      "Epoch 127: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5405728816986084\n",
      "Epoch 128: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5400673747062683\n",
      "Epoch 129: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5402538180351257\n",
      "Epoch 130: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5396128296852112\n",
      "Epoch 131: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5401164293289185\n",
      "Epoch 132: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5403177738189697\n",
      "Epoch 133: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5409425497055054\n",
      "Epoch 134: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5403428673744202\n",
      "Epoch 135: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.539694607257843\n",
      "Epoch 136: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5408427119255066\n",
      "Epoch 137: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5452724695205688\n",
      "Epoch 138: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5424765944480896\n",
      "Epoch 139: individual accs:[[0.9783549783549783, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9191919191919192 loss:0.5430141091346741\n",
      "Epoch 140: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.540901780128479\n",
      "Epoch 141: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5406398177146912\n",
      "Epoch 142: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5400556921958923\n",
      "Epoch 143: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5404992699623108\n",
      "Epoch 144: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.540851891040802\n",
      "Epoch 145: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5410894155502319\n",
      "Epoch 146: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5409928560256958\n",
      "Epoch 147: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5409263372421265\n",
      "Epoch 148: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5428288578987122\n",
      "Epoch 149: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5408148169517517\n",
      "Epoch 150: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5420462489128113\n",
      "Epoch 151: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5409013032913208\n",
      "Epoch 152: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5406926870346069\n",
      "Epoch 153: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5396596193313599\n",
      "Epoch 154: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.539987325668335\n",
      "Epoch 155: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5408331751823425\n",
      "Epoch 156: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5398107767105103\n",
      "Epoch 157: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.539678692817688\n",
      "Epoch 158: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5400350093841553\n",
      "Epoch 159: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5404988527297974\n",
      "Epoch 160: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5399510264396667\n",
      "Epoch 161: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5397752523422241\n",
      "Epoch 162: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5403976440429688\n",
      "Epoch 163: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5401859283447266\n",
      "Epoch 164: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5402411222457886\n",
      "Epoch 165: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.539380669593811\n",
      "Epoch 166: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5389140248298645\n",
      "Epoch 167: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5387952327728271\n",
      "Epoch 168: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5390666127204895\n",
      "Epoch 169: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5393074750900269\n",
      "Epoch 170: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5389467477798462\n",
      "Epoch 171: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5387632250785828\n",
      "Epoch 172: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5385357141494751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5385018587112427\n",
      "Epoch 174: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5391882061958313\n",
      "Epoch 175: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5391044616699219\n",
      "Epoch 176: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5384325981140137\n",
      "Epoch 177: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5402746200561523\n",
      "Epoch 178: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.539568305015564\n",
      "Epoch 179: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5385193824768066\n",
      "Epoch 180: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5384078025817871\n",
      "Epoch 181: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.538265585899353\n",
      "Epoch 182: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5378599762916565\n",
      "Epoch 183: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5376091003417969\n",
      "Epoch 184: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5375916957855225\n",
      "Epoch 185: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5377101898193359\n",
      "Epoch 186: individual accs:[[0.9826839826839827, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9206349206349206 loss:0.5392064452171326\n",
      "Epoch 187: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5366727113723755\n",
      "Epoch 188: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5358223915100098\n",
      "Epoch 189: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5355162024497986\n",
      "Epoch 190: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5353546738624573\n",
      "Epoch 191: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5351011753082275\n",
      "Epoch 192: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5351188778877258\n",
      "Epoch 193: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.534817636013031\n",
      "Epoch 194: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5350281596183777\n",
      "Epoch 195: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5351409316062927\n",
      "Epoch 196: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.535994827747345\n",
      "Epoch 197: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5370908975601196\n",
      "Epoch 198: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5364317297935486\n",
      "Epoch 199: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5361063480377197\n",
      "Epoch 200: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5360792279243469\n",
      "Epoch 201: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5358532667160034\n",
      "Epoch 202: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5351486206054688\n",
      "Epoch 203: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.534633219242096\n",
      "Epoch 204: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5344710350036621\n",
      "Epoch 205: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.534773051738739\n",
      "Epoch 206: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5345005393028259\n",
      "Epoch 207: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5343674421310425\n",
      "Epoch 208: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5343053340911865\n",
      "Epoch 209: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5342360138893127\n",
      "Epoch 210: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5343403220176697\n",
      "Epoch 211: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5338829755783081\n",
      "Epoch 212: individual accs:[[0.9913419913419913, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9235209235209235 loss:0.5353392958641052\n",
      "Epoch 213: individual accs:[[0.9393939393939394, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9062049062049061 loss:0.5473974943161011\n",
      "Epoch 214: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5387933254241943\n",
      "Epoch 215: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5368369221687317\n",
      "Epoch 216: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5346179604530334\n",
      "Epoch 217: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5339258909225464\n",
      "Epoch 218: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5335831046104431\n",
      "Epoch 219: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5334877371788025\n",
      "Epoch 220: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5335006713867188\n",
      "Epoch 221: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.533948540687561\n",
      "Epoch 222: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5339882969856262\n",
      "Epoch 223: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5337164402008057\n",
      "Epoch 224: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5337662100791931\n",
      "Epoch 225: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5359726548194885\n",
      "Epoch 226: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5342469811439514\n",
      "Epoch 227: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.534456729888916\n",
      "Epoch 228: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5346015691757202\n",
      "Epoch 229: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5345652103424072\n",
      "Epoch 230: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5341320037841797\n",
      "Epoch 231: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5337986946105957\n",
      "Epoch 232: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5335686206817627\n",
      "Epoch 233: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.533575713634491\n",
      "Epoch 234: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5342077016830444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5342140197753906\n",
      "Epoch 236: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5341496467590332\n",
      "Epoch 237: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5335543155670166\n",
      "Epoch 238: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5332558155059814\n",
      "Epoch 239: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.533170223236084\n",
      "Epoch 240: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5332999229431152\n",
      "Epoch 241: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.532660722732544\n",
      "Epoch 242: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.53629070520401\n",
      "Epoch 243: individual accs:[[0.9956709956709957, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.924963924963925 loss:0.5377907156944275\n",
      "Epoch 244: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5344402194023132\n",
      "Epoch 245: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5335469841957092\n",
      "Epoch 246: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5332911610603333\n",
      "Epoch 247: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5333693623542786\n",
      "Epoch 248: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.5331145524978638\n",
      "Epoch 249: individual accs:[[1.0, 0.8917748917748918, 0.8874458874458875]] avg accuracy 0.9264069264069263 loss:0.533415675163269\n",
      "[0.61339515, 0.583557, 0.5760027, 0.5719767, 0.56934804, 0.56764793, 0.567223, 0.5673641, 0.5668347, 0.56614983, 0.5657148, 0.5654608, 0.5650544, 0.5647258, 0.5647971, 0.5645955, 0.5644248, 0.56412375, 0.5641341, 0.5642467, 0.564021, 0.5638037, 0.5634368, 0.5633376, 0.5634286, 0.56316715, 0.5632122, 0.56318825, 0.5630187, 0.5633848, 0.5635497, 0.56352127, 0.5634737, 0.56365824, 0.563471, 0.56337214, 0.5641279, 0.5632875, 0.564116, 0.56417334, 0.5638094, 0.56402266, 0.5645917, 0.5627801, 0.56263953, 0.5627961, 0.56346685, 0.56288, 0.56069624, 0.5599171, 0.5598998, 0.55929935, 0.5583043, 0.5580455, 0.5580595, 0.5573692, 0.5568398, 0.5567858, 0.5549907, 0.55503666, 0.554269, 0.5546345, 0.5529546, 0.55306506, 0.5548505, 0.5547096, 0.5521226, 0.55161613, 0.54997325, 0.54999256, 0.54866034, 0.5487365, 0.54882145, 0.5474055, 0.5472192, 0.546538, 0.54654366, 0.5467812, 0.5469466, 0.5464629, 0.54526633, 0.546912, 0.5465551, 0.5449363, 0.5452035, 0.54480505, 0.5450688, 0.5451873, 0.54558235, 0.5447055, 0.5450198, 0.5465601, 0.54455703, 0.5438414, 0.5436744, 0.5437962, 0.5440733, 0.5435831, 0.5440599, 0.542148, 0.5421749, 0.54270256, 0.5417293, 0.54211, 0.54282546, 0.5427303, 0.541611, 0.54114234, 0.541576, 0.54145944, 0.5417073, 0.54117537, 0.54102784, 0.5414661, 0.5417713, 0.5406211, 0.54093283, 0.5412972, 0.54125637, 0.5405208, 0.54044104, 0.5400532, 0.54022914, 0.5402692, 0.5407341, 0.5421977, 0.54081315, 0.5405729, 0.5400674, 0.5402538, 0.5396128, 0.5401164, 0.5403178, 0.54094255, 0.54034287, 0.5396946, 0.5408427, 0.54527247, 0.5424766, 0.5430141, 0.5409018, 0.5406398, 0.5400557, 0.54049927, 0.5408519, 0.5410894, 0.54099286, 0.54092634, 0.54282886, 0.5408148, 0.54204625, 0.5409013, 0.5406927, 0.5396596, 0.5399873, 0.5408332, 0.5398108, 0.5396787, 0.540035, 0.54049885, 0.539951, 0.53977525, 0.54039764, 0.5401859, 0.5402411, 0.53938067, 0.538914, 0.53879523, 0.5390666, 0.5393075, 0.53894675, 0.5387632, 0.5385357, 0.53850186, 0.5391882, 0.53910446, 0.5384326, 0.5402746, 0.5395683, 0.5385194, 0.5384078, 0.5382656, 0.53786, 0.5376091, 0.5375917, 0.5377102, 0.53920645, 0.5366727, 0.5358224, 0.5355162, 0.5353547, 0.5351012, 0.5351189, 0.53481764, 0.53502816, 0.53514093, 0.5359948, 0.5370909, 0.5364317, 0.53610635, 0.5360792, 0.53585327, 0.5351486, 0.5346332, 0.53447104, 0.53477305, 0.53450054, 0.53436744, 0.53430533, 0.534236, 0.5343403, 0.533883, 0.5353393, 0.5473975, 0.5387933, 0.5368369, 0.53461796, 0.5339259, 0.5335831, 0.53348774, 0.5335007, 0.53394854, 0.5339883, 0.53371644, 0.5337662, 0.53597265, 0.534247, 0.53445673, 0.53460157, 0.5345652, 0.534132, 0.5337987, 0.5335686, 0.5335757, 0.5342077, 0.534214, 0.53414965, 0.5335543, 0.5332558, 0.5331702, 0.5332999, 0.5326607, 0.5362907, 0.5377907, 0.5344402, 0.533547, 0.53329116, 0.53336936, 0.53311455, 0.5334157]\n",
      "[0.8181818181818182, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8210678210678211, 0.8383838383838383, 0.8470418470418469, 0.8585858585858586, 0.8441558441558442, 0.86002886002886, 0.86002886002886, 0.8556998556998557, 0.86002886002886, 0.8701298701298702, 0.8744588744588745, 0.888888888888889, 0.8874458874458875, 0.8989898989898991, 0.8989898989898991, 0.9004329004329005, 0.9076479076479077, 0.9105339105339105, 0.9062049062049061, 0.9090909090909092, 0.9033189033189033, 0.9062049062049061, 0.9134199134199134, 0.9134199134199134, 0.911976911976912, 0.9090909090909092, 0.9191919191919192, 0.9191919191919192, 0.9191919191919192, 0.9206349206349206, 0.9206349206349206, 0.9191919191919192, 0.9177489177489178, 0.9191919191919192, 0.9163059163059163, 0.9163059163059163, 0.9191919191919192, 0.9206349206349206, 0.9163059163059163, 0.9206349206349206, 0.9206349206349206, 0.9177489177489178, 0.9235209235209235, 0.9235209235209235, 0.9206349206349206, 0.9220779220779222, 0.9235209235209235, 0.9191919191919192, 0.9220779220779222, 0.9220779220779222, 0.9235209235209235, 0.9220779220779222, 0.9220779220779222, 0.9235209235209235, 0.9235209235209235, 0.9235209235209235, 0.9235209235209235, 0.9191919191919192, 0.924963924963925, 0.9206349206349206, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.9235209235209235, 0.9235209235209235, 0.924963924963925, 0.924963924963925, 0.9220779220779222, 0.9220779220779222, 0.9220779220779222, 0.9235209235209235, 0.9235209235209235, 0.9235209235209235, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.9235209235209235, 0.9191919191919192, 0.9235209235209235, 0.9191919191919192, 0.924963924963925, 0.924963924963925, 0.9264069264069263, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.9235209235209235, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.924963924963925, 0.9264069264069263, 0.924963924963925, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.924963924963925, 0.9264069264069263, 0.9264069264069263, 0.924963924963925, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.924963924963925, 0.9264069264069263, 0.9206349206349206, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.924963924963925, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9235209235209235, 0.9062049062049061, 0.924963924963925, 0.924963924963925, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.924963924963925, 0.924963924963925, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263, 0.9264069264069263]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_accuracies  = []\n",
    "total_losses = []\n",
    "\n",
    "conv_lstm.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    \n",
    "    epoch_predictions = []\n",
    "    epoch_real_values = []\n",
    "    epoch_losses = []\n",
    "    # en cada epoch procesar los pacientes en orden aleatorio\n",
    "    pacientes = np.random.choice(np.array(diagnosticos.paciente),size= len(diagnosticos.paciente),replace=False)\n",
    "    \n",
    "    conteo_pacientes = 0\n",
    "    for paciente in pacientes:\n",
    "        #TODO: remover esta validacion, solo puesta para probar una unica iteracion en compu lenta\n",
    "        if conteo_pacientes >= 99999999:\n",
    "            break\n",
    "            \n",
    "        #TODO: this patient has many images and creates an out of memory error\n",
    "        if  len(diccionario_imagenes_pacientes.get(paciente)) >= 50:\n",
    "            continue\n",
    "            \n",
    "        h = conv_lstm.init_hidden()\n",
    "        \n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        conv_lstm.zero_grad()\n",
    "        \n",
    "        imagenes_paciente = diccionario_imagenes_pacientes.get(paciente)\n",
    "        diagnostico_hemorragia_paciente = np.array(float(diagnosticos[diagnosticos.paciente==paciente].hemorragia))\n",
    "        vector_diagnostico_paciente = np.array(diagnosticos[diagnosticos.paciente==paciente][[\"hemorragia\",\"isquemia\",\"fractura\"]])\n",
    "        tensor_diagnostico_paciente = torch.Tensor(vector_diagnostico_paciente).view((1,3)).to(device)\n",
    "        \n",
    "        tensor_diagnostico_hemorragia_paciente = torch.Tensor(diagnostico_hemorragia_paciente).view((1,1)).to(device)\n",
    "        \n",
    "        indices_imagenes_pacientes = np.arange(0,len(imagenes_paciente)-1,step=1)\n",
    "        indices_aleatorios_imagenes = np.random.choice(indices_imagenes_pacientes,len(indices_imagenes_pacientes),replace=False)\n",
    "        \n",
    "        losses = []\n",
    "        for indice in indices_aleatorios_imagenes:\n",
    "            #h = tuple([each.data for each in h])\n",
    "            #print(paciente,indice)\n",
    "            last_image =  int(indice  == indices_aleatorios_imagenes[-1])\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            \n",
    "            imagen_paciente =  np.expand_dims(imagenes_paciente[indice],2)\n",
    "            imagen_paciente =  np.repeat(imagen_paciente,3,axis=2)\n",
    "               \n",
    "            tensor_imagen_paciente =  train_data_transforms(imagen_paciente).unsqueeze(0).to(device)\n",
    "            \n",
    "            \n",
    "            #print(\"Alimentando paciente {} e imagen {} al modelo\".format(paciente,indice),imagen_paciente.shape)\n",
    "            \n",
    "            diagnostico_aproximado,h  = conv_lstm(tensor_imagen_paciente,h)\n",
    "            prob_diagnostico_aproximado = torch.sigmoid(diagnostico_aproximado)\n",
    "            diagnostico_hemorragia_aproximado = diagnostico_aproximado[:,0]\n",
    "            \n",
    "            \n",
    "            #loss = base_criterion(diagnostico_hemorragia_aproximado,tensor_diagnostico_hemorragia_paciente)\n",
    "            \n",
    "            #losses.append(loss.view((1,1)))\n",
    "            \n",
    "            if last_image:\n",
    "                \n",
    "                loss =  base_criterion(diagnostico_aproximado,tensor_diagnostico_paciente)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(conv_lstm.lstm.parameters(), 5.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                vector_y_hat_hard = prob_diagnostico_aproximado >= 0.5\n",
    "                \n",
    "                #print(torch.sigmoid(diagnostico_aproximado).data,tensor_diagnostico_paciente.data)\n",
    "                y_hat = diagnostico_hemorragia_aproximado.data.cpu().numpy()[0]\n",
    "                y_hat_hard = float(y_hat >= 0.5)\n",
    "                \n",
    "                epoch_predictions.append(vector_y_hat_hard.data.cpu().numpy()[0])\n",
    "                epoch_real_values.append(vector_diagnostico_paciente[0])\n",
    "                \n",
    "                \n",
    "                #print(\"--Flag ultima imagen:{} diagnostico:{} valor real{}\".format(last_image,y_hat,diagnostico_hemorragia_paciente))\n",
    "                #optimizer.zero_grad()\n",
    "                \n",
    "                #patient_loss = torch.cat(losses).mean()\n",
    "                \n",
    "                \n",
    "                #patient_loss.backward()\n",
    "                #loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(conv_lstm.lstm.parameters(), 5.0)\n",
    "                #optimizer.step()\n",
    "                \n",
    "                epoch_losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "                \n",
    "        conteo_pacientes += 1\n",
    "        \n",
    "            \n",
    "    #epoch_predictions = np.array(epoch_predictions)\n",
    "    #epoch_real_values = np.array(epoch_real_values)\n",
    "    #correct_predictions = epoch_predictions == epoch_real_values\n",
    "    #accuracy = np.average(correct_predictions)\n",
    "    accuracies,average_accuracy = calc_accuracy(np.array(epoch_predictions),np.array(epoch_real_values))\n",
    "    \n",
    "    epoch_avg_loss = np.average(epoch_losses)\n",
    "    total_losses.append(epoch_avg_loss)\n",
    "    \n",
    "    total_accuracies.append(average_accuracy)\n",
    "    print(\"Epoch {}: individual accs:[{}] avg accuracy {} loss:{}\".format(epoch,accuracies,average_accuracy,epoch_avg_loss))\n",
    "\n",
    "#print(np.average(total_accuracies))\n",
    "print(total_losses)\n",
    "print(total_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "* El mejor resultado hasta ahora ha sido no congelar pesos(entranar toda la convnet densenet121) agregandole una sola capa fully connected de salida, y 3 layers en la lstm, todas las capas con 1024 de tamaño. Lr = 0.001.\n",
    "* Congelando las primeras 50 capas de la convnet converge alrededor de los 40 epochs(pero sigue bajando) con la misma configuración qeu el resultado 1.\n",
    "* Misma arquitectura pero congelando 100 capas de la convnet(y agregando una nueva muestra de pacientes de 3 ) converge alrededor de los 25 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "* Normalizar  el allocation weighitng con sofmax(en la primera iteración asigna todo el peso a la primera posición de memoria)\n",
    "* Usar arquitectura similar a dueling network o inception para tener 2 caminos en las entradas.\n",
    "* Cambiar el modelo original para leer antes que escribir y usar lo leido para sacar una predicción en ese punto en el tiempo(el modelo original lee de la memoria despues de escribir y usa la info leida en el siguiente paso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

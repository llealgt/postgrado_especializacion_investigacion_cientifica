{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.15</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Warning: You are not running the latest version of PixieDust. Current is 1.1.15, Latest is 1.1.17</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>Please copy and run the following command in a new cell to upgrade: <span style=\"background-color:#ececec;font-family:monospace;padding:0 5px\">!pip install --user --upgrade pixiedust</span></div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Please restart kernel after upgrading.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pixiedust\n",
    "from torchvision import transforms,datasets,models\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas y recordatorios\n",
    "\n",
    "* Por la forma en que se prepararon los datos renombre  temporalmente el paciente 120 a paciente 6(para mantener la continuidad de la muestra) y lo puse tambi√©n como 6 en los diagnosticos.\n",
    "* Similar al punto anterior pero con el paciente 121 y el 7\n",
    "* Similar pero con el paciente 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fast experimentation on slow computer limit the size of the sample\n",
    "MAX_PATIENTS = 800 \n",
    "MINI_BATCH_SIZE = 32 #curently not used for mini batch split but for deciding when to do backward and zero grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../datos/TOMOGRAFIAS/\"\n",
    "VALIDATION_FRACTION  = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosticos  = pd.read_excel(DATA_DIRECTORY+\"RESUMEN TAC CEREBRALES.xlsx\")\n",
    "nombres_diagnosticos = diagnosticos.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paciente         1142\n",
       "hemorragia        907\n",
       "isquemia          907\n",
       "fractura          907\n",
       "masa              907\n",
       "edema             906\n",
       "observaciones     157\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosticos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paciente</th>\n",
       "      <th>hemorragia</th>\n",
       "      <th>isquemia</th>\n",
       "      <th>fractura</th>\n",
       "      <th>masa</th>\n",
       "      <th>edema</th>\n",
       "      <th>observaciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>torax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>urotac revisar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no est√°n las im√°genes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paciente  hemorragia  isquemia  fractura  masa  edema  \\\n",
       "0          1         1.0       0.0       0.0   0.0    0.0   \n",
       "1          2         1.0       0.0       1.0   0.0    1.0   \n",
       "2          3         1.0       0.0       1.0   0.0    0.0   \n",
       "3          4         0.0       1.0       0.0   0.0    0.0   \n",
       "4          5         0.0       0.0       0.0   1.0    1.0   \n",
       "5          6         1.0       0.0       0.0   0.0    0.0   \n",
       "6          7         0.0       0.0       0.0   0.0    0.0   \n",
       "7          8         0.0       0.0       0.0   0.0    0.0   \n",
       "8          9         0.0       0.0       0.0   0.0    0.0   \n",
       "9         10         0.0       0.0       0.0   0.0    0.0   \n",
       "10        11         0.0       0.0       0.0   0.0    0.0   \n",
       "11        12         0.0       0.0       0.0   0.0    0.0   \n",
       "12        13         NaN       NaN       NaN   NaN    NaN   \n",
       "13        14         NaN       NaN       NaN   NaN    NaN   \n",
       "14        15         1.0       0.0       1.0   0.0    0.0   \n",
       "15        16         NaN       NaN       NaN   NaN    NaN   \n",
       "16        17         1.0       0.0       1.0   0.0    1.0   \n",
       "17        18         0.0       0.0       0.0   0.0    0.0   \n",
       "18        19         1.0       0.0       0.0   0.0    0.0   \n",
       "19        20         0.0       0.0       0.0   0.0    0.0   \n",
       "\n",
       "            observaciones  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "5                     NaN  \n",
       "6                     NaN  \n",
       "7                     NaN  \n",
       "8                     NaN  \n",
       "9                     NaN  \n",
       "10                    NaN  \n",
       "11                    NaN  \n",
       "12                  torax  \n",
       "13         urotac revisar  \n",
       "14                    NaN  \n",
       "15                abdomen  \n",
       "16                    NaN  \n",
       "17                    NaN  \n",
       "18  no est√°n las im√°genes  \n",
       "19                    NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosticos.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diccionario cuya llave es el id de paciente y el valor una lista \n",
    "# donde cada elemento de la lista es la matriz de una i\n",
    "diccionario_imagenes_pacientes = dict()\n",
    "processed_patients = 0\n",
    "\n",
    "for paciente in diagnosticos.paciente:\n",
    "    if processed_patients >= MAX_PATIENTS:\n",
    "        diagnosticos = diagnosticos.iloc[:processed_patients]\n",
    "        break\n",
    "    directorio_paciente = DATA_DIRECTORY+\"paciente_\"+str(paciente)\n",
    "    \n",
    "    # if patient directory is missing OR any of the diagnostics is null \n",
    "    #do not try to read images and delete it from diagnostics dataframe\n",
    "    if not os.path.exists(directorio_paciente) or  diagnosticos[diagnosticos.paciente == paciente].iloc[:,0:6].isnull().values.any():\n",
    "        diagnostics_row = diagnosticos[diagnosticos.paciente == paciente]\n",
    "        diagnosticos.drop(int(diagnostics_row.index.values),axis=0,inplace=True)\n",
    "        \n",
    "        continue\n",
    "    archivos_paciente = os.listdir(directorio_paciente)\n",
    "    \n",
    "    if len(archivos_paciente)>= 50:\n",
    "        diagnostics_row = diagnosticos[diagnosticos.paciente == paciente]\n",
    "        diagnosticos.drop(int(diagnostics_row.index.values),axis=0,inplace=True)\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    lista_imagenes_paciente = []\n",
    "    for archivo in archivos_paciente:\n",
    "        if archivo.endswith(\".jpg\"):\n",
    "            imagen = mpimg.imread(directorio_paciente+\"/\"+archivo)\n",
    "            lista_imagenes_paciente.append(imagen)\n",
    "            \n",
    "    processed_patients += 1\n",
    "            \n",
    "    diccionario_imagenes_pacientes[paciente] = lista_imagenes_paciente\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pacientes: 715\n"
     ]
    }
   ],
   "source": [
    "print(\"Total pacientes:\",processed_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paciente</th>\n",
       "      <th>hemorragia</th>\n",
       "      <th>isquemia</th>\n",
       "      <th>fractura</th>\n",
       "      <th>masa</th>\n",
       "      <th>edema</th>\n",
       "      <th>observaciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paciente  hemorragia  isquemia  fractura  masa  edema observaciones\n",
       "0          1         1.0       0.0       0.0   0.0    0.0           NaN\n",
       "1          2         1.0       0.0       1.0   0.0    1.0           NaN\n",
       "2          3         1.0       0.0       1.0   0.0    0.0           NaN\n",
       "3          4         0.0       1.0       0.0   0.0    0.0           NaN\n",
       "4          5         0.0       0.0       0.0   1.0    1.0           NaN\n",
       "5          6         1.0       0.0       0.0   0.0    0.0           NaN\n",
       "6          7         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "7          8         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "8          9         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "9         10         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "10        11         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "11        12         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "16        17         1.0       0.0       1.0   0.0    1.0           NaN\n",
       "17        18         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "19        20         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "20        21         1.0       0.0       0.0   0.0    0.0           NaN\n",
       "21        22         1.0       0.0       1.0   0.0    1.0           NaN\n",
       "22        23         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "23        24         0.0       0.0       0.0   0.0    0.0           NaN\n",
       "24        25         0.0       0.0       0.0   0.0    0.0           NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosticos.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separacion \"train-val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonsticos_val = diagnosticos.sample(frac=VALIDATION_FRACTION)\n",
    "diccionario_imagenes_pacientes_val = {llave:diccionario_imagenes_pacientes[llave] for llave in diagonsticos_val.paciente}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosticos = diagnosticos[~diagnosticos.paciente.isin(diagonsticos_val.paciente)]\n",
    "diccionario_imagenes_pacientes = {llave:diccionario_imagenes_pacientes[llave] for llave in diagnosticos.paciente}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 643\n",
      "Val size: 72\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\",len(diagnosticos))\n",
    "print(\"Val size:\",len(diagonsticos_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporcion de positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hemorragia    0.283048\n",
       "isquemia      0.102644\n",
       "fractura      0.073095\n",
       "masa          0.018663\n",
       "edema         0.153966\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosticos.iloc[:,1:-1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hemorragia    0.291667\n",
       "isquemia      0.138889\n",
       "fractura      0.083333\n",
       "masa          0.013889\n",
       "edema         0.097222\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagonsticos_val.iloc[:,1:-1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos y arquitecturas\n",
    "### Arquitecturas experimental  DNC\n",
    "* Alimentamos al modelo imagen por imagen y se presenta un solo diagnostico por paciente\n",
    "* El controller de la DNC esta compuesto por una convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_OUTPUT_SIZE = 128\n",
    "READ_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: cambiar valores quemados por valores parametrizados y calculos dependientes\n",
    "class ConvController(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,4,kernel_size=3,stride=1)\n",
    "        self.fc1  =  torch.nn.Linear(262144,CONTROLLER_OUTPUT_SIZE)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        h = self.conv1(x)\n",
    "        \n",
    "        #flatten\n",
    "        h =  x.view(-1,x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        h =  self.fc1(h)\n",
    "        \n",
    "        return h #h_t in my txt\n",
    "    \n",
    "class Controller(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_controller = ConvController()\n",
    "        self.fc1 = torch.nn.Linear(10,CONTROLLER_OUTPUT_SIZE)\n",
    "        self.fc2 = torch.nn.Linear(2*CONTROLLER_OUTPUT_SIZE,CONTROLLER_OUTPUT_SIZE)\n",
    "        \n",
    "    def forward(self,x,read_vectors):\n",
    "        h_conv = self.conv_controller(x)\n",
    "        h_read_vectors = self.fc1(read_vectors)\n",
    "        \n",
    "        h_t = torch.cat((h_conv,h_read_vectors),dim=1)\n",
    "        \n",
    "        h_t =  torch.relu( h_t)\n",
    "        h_t =  self.fc2(h_t) \n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: cambiar valores quemados por valores parametrizados y calculos dependientes\n",
    "#TODO: cordar por que en algun momento le puse bias = False a los pesos del vector de salida de la DNC\n",
    "\n",
    "\n",
    "class DNC(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,controller,memory_size = (10,10),read_heads = 1,device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.controller = controller\n",
    "        self.device = device\n",
    "        self.N = memory_size[0] # number of memory locations\n",
    "        self.W = memory_size[1] # word size of the memory \n",
    "        self.R = read_heads # number of read heads\n",
    "        self.WS = 1 #not in the paper(they use 1), but used as a parametrizable number of write heads for further experiments\n",
    "        self.interface_vector_size = (self.W*self.R) + (self.W*self.WS) + (2*self.W) + (5*self.R) + 3\n",
    "        \n",
    "        # inicialization st to random just for testing, remember to put on zeros\n",
    "        #self.memory_matrix = self.memory_matrix =  nn.Parameter(torch.zeros(size=memory_size),requires_grad= False) \n",
    "        \n",
    "        #1024 es el tama√±o del vector de salida del controlador, 1 es el tama√±o de salida de la dnc\n",
    "        self.output_vector_linear = torch.nn.Linear(CONTROLLER_OUTPUT_SIZE,1,bias=True) #W_y \n",
    "        self.interface_vector_linear = torch.nn.Linear(CONTROLLER_OUTPUT_SIZE,self.interface_vector_size,bias=True) #W_Œæ\n",
    "        self.read_vectors_to_output_linear = torch.nn.Linear(self.R*self.W,1,bias = True) #W_r in my txt\n",
    "        \n",
    "        self.read_keys = torch.Tensor(size=(self.R,self.W)).requires_grad_(False) # k_r in my txt\n",
    "        self.read_strenghts = torch.Tensor(size=(self.R,1)).requires_grad_(False) #Œ≤_r\n",
    "        \n",
    "        #self.read_weighting = torch.Tensor(torch.zeros(size=(self.R,self.N))).requires_grad_(False).to(device) #r_w\n",
    "        \n",
    "        self.write_key = torch.Tensor(size=(1,self.W)).requires_grad_(False) # k_w in my txt\n",
    "        self.write_strenght = torch.Tensor(size=(1,1)).requires_grad_(False) # Œ≤_w\n",
    "        \n",
    "        #self.write_weighting = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(False) # w_w\n",
    "        \n",
    "        #self.usage_vector = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(False) #u_t\n",
    "        \n",
    "        self.memory_matrix_ones = torch.Tensor(torch.ones(size=memory_size)).requires_grad_(True).to(device) #E on paper\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def forward(self,x,read_vectors):\n",
    "        \n",
    "        h_t = self.controller(x,read_vectors) #controller output called ht in the paper\n",
    "        \n",
    "        output_vector = self.output_vector_linear(h_t) # called Vt in the paper(œÖ=Wy[h1;...;hL]) v_o_t in my txt\n",
    "        interface_vector = self.interface_vector_linear(h_t).data #called Œæt(ksi) in the paper ,Œæ_t in my txt\n",
    "        \n",
    "        self.read_keys.data = interface_vector[0,0:self.R*self.W].view((self.R,self.W)) #k_r in my txt\n",
    "        \n",
    "        #clamp temporary added because the exp was returning inf  values\n",
    "        read_strenghts =  torch.clamp( interface_vector[0,self.R*self.W:self.R*self.W+self.R].view((self.R,1)),max=85)\n",
    "        self.read_strenghts.data = self.oneplus(read_strenghts) #Œ≤_r\n",
    "        \n",
    "        self.write_key.data = interface_vector[0,self.R*self.W+self.R:self.R*self.W+self.R+self.W].view((1,self.W)) # k_w\n",
    "        \n",
    "        write_strenght = torch.clamp(interface_vector[:,self.R*self.W+self.R+self.W:self.R*self.W+self.R+self.W + 1].view((1,1)),max=85)\n",
    "        self.write_strenght.data = self.oneplus(write_strenght) #Œ≤_w\n",
    "        \n",
    "        erase_vector = interface_vector[0,self.R*self.W+self.R+self.W + 1: self.R*self.W+self.R+self.W + 1 + self.W].view((1,self.W))\n",
    "        erase_vector = torch.sigmoid(erase_vector) #e_t\n",
    "        \n",
    "        write_vector = interface_vector[0,self.R*self.W+self.R+self.W + 1 + self.W:self.R*self.W+self.R+self.W + 1 + 2*self.W].view((1,self.W)) #v_t\n",
    "        \n",
    "        free_gates  =  interface_vector[0,self.R*self.W+self.R+self.W + 1 + 2*self.W:self.R*self.W+2*self.R+self.W + 1 + 2*self.W].view((self.R,1)) #f_t\n",
    "        free_gates =   torch.sigmoid(free_gates)\n",
    "        \n",
    "        allocation_gate = interface_vector[0,self.R*self.W+2*self.R+self.W + 1 + 2*self.W:self.R*self.W+2*self.R+self.W + 1 + 2*self.W+1]\n",
    "        allocation_gate = torch.sigmoid(allocation_gate)\n",
    "        \n",
    "        write_gate = interface_vector[0,self.R*self.W+2*self.R+self.W + 1 + 2*self.W+1:self.R*self.W+2*self.R+self.W + 1 + 2*self.W+2]\n",
    "        write_gate = torch.sigmoid( write_gate)\n",
    "        \n",
    "        \n",
    "        # Escritura\n",
    "        # TODO: verificar y/o experimentar si el ordern es :primero escribir y luego leer de la memoria(asi parece en el pazper)\n",
    "        retention_vector = (1.0 - free_gates * self.read_weighting).prod(dim=0)\n",
    "        self.usage_vector.data = (self.usage_vector +self.write_weighting - (self.usage_vector *self.write_weighting))*retention_vector #u_t\n",
    "        allocation_weighting = self.calc_allocation_weighting(self.usage_vector)\n",
    "        write_content_weighting = self.content_lookup(self.memory_matrix,self.write_key,self.write_strenght)\n",
    "\n",
    "        self.write_weighting.data =  write_gate*(  \n",
    "            (allocation_gate * allocation_weighting) +  ((1- allocation_gate)*write_content_weighting))\n",
    "        \n",
    "        new_memory_matrix = self.memory_matrix*(self.memory_matrix_ones - torch.matmul(self.write_weighting.t(),erase_vector)) + torch.matmul(self.write_weighting.t(),write_vector)\n",
    "        \n",
    "        self.memory_matrix.data = new_memory_matrix\n",
    "        \n",
    "        # read by content weithing(attention by similarity)\n",
    "        read_content_weighting = self.content_lookup(self.memory_matrix,self.read_keys,self.read_strenghts)\n",
    "        \n",
    "        #read weithing is a combination of reading modes,TODO:add temporal attention not just by similarity\n",
    "        self.read_weighting.data = read_content_weighting\n",
    "        \n",
    "        read_vectors = torch.matmul(self.read_weighting,self.memory_matrix).view((1,self.R*self.W)) #r in my txt\n",
    "        read_heads_to_output = self.read_vectors_to_output_linear(read_vectors) #v_r_t in my t xt\n",
    "        \n",
    "        #TODO: experiment and decide if maintain sigmoid\n",
    "        y_t = torch.sigmoid(output_vector + read_heads_to_output)\n",
    "        return y_t,read_vectors\n",
    "    \n",
    "    def oneplus(self,x):\n",
    "        # apply oneplus operation to a tensor to constrain it's elements to [1,inf)\n",
    "        #TODO: check numerical statiliby as exp is returning inf for numbers like 710,emporary added clamp to 85\n",
    "        return torch.log(1+torch.exp(x)) + 1\n",
    "    \n",
    "    def content_lookup(self,matrix,keys,strengths):\n",
    "        # returns a probability distribution over the memory locations \n",
    "        # with higher probability to memory locations with bigger similarity to the keys\n",
    "        # bigger strenght make more aggresive distributions ,for example a distribution (0.2,0.3,0.5) with\n",
    "        # bigger strenght becomes (0.1,0.12,0.78)\n",
    "        # returns tensor of shape (read keys,memory size) = (R,N)\n",
    "        keys_norm =  torch.sqrt(torch.sum(keys**2,dim=1).unsqueeze(dim=1))\n",
    "        matrix_norm = torch.sqrt(torch.sum(matrix**2,dim=1))\n",
    "        norms_multiplication = keys_norm*matrix_norm\n",
    "        # calc cosine similarity between keys and memory locations(1e-6 is used avoiding div by 0)\n",
    "        divide_zero_prevent_factor = torch.zeros_like(norms_multiplication).add_(1e-6)\n",
    "        cosine_similarity = torch.matmul(keys,matrix.t())/(torch.max(norms_multiplication,divide_zero_prevent_factor))\n",
    "        \n",
    "        # do a \"strenght\" softmax to calculate the probability distribution\n",
    "        numerator = torch.exp(cosine_similarity*strengths)\n",
    "        denominator = numerator.sum(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "        distribution = numerator/denominator\n",
    "        \n",
    "        return distribution\n",
    "    \n",
    "    def calc_allocation_weighting(self,usage_vector):\n",
    "        #print(\"usage vector\",usage_vector)\n",
    "        _,free_list = torch.topk(-usage_vector,self.N,dim=1) #œÜt indices of memory locations ordered by usage\n",
    "        #print(\"free list\",free_list)\n",
    "        free_list = free_list.view(-1)\n",
    "        #print(\"reshaped free list\",free_list)\n",
    "        _,ordered_free_list =  torch.topk(-free_list,self.N)\n",
    "        ordered_free_list = ordered_free_list.view(-1)\n",
    "        #print(\"ordered free list\",ordered_free_list)\n",
    "        ordered_usage_vector = usage_vector[:,free_list]\n",
    "        #print(\"ordered usage vector\",ordered_usage_vector)\n",
    "        ordered_usage_vector_cumulative_product = torch.ones(size=(1,self.N+1)).to(device)\n",
    "        #print(ordered_usage_vector_cumulative_product)\n",
    "        #print(\"cumprod \",ordered_usage_vector.cumprod(dim=1))\n",
    "        ordered_usage_vector_cumulative_product[0,1:] = ordered_usage_vector.cumprod(dim=1)\n",
    "        #print(ordered_usage_vector_cumulative_product)\n",
    "        \n",
    "        allocation_weighting = (1 - usage_vector)*ordered_usage_vector_cumulative_product[0,ordered_free_list]\n",
    "        \n",
    "        return  allocation_weighting\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory_matrix =  torch.Tensor(torch.zeros(size=(self.N,self.W))).requires_grad_(True).to(device) \n",
    "        self.read_weighting = torch.Tensor(torch.zeros(size=(self.R,self.N))).requires_grad_(True).to(device) #r_w\n",
    "        self.write_weighting = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(True).to(device) # w_w\n",
    "        self.usage_vector = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(True).to(device) #u_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos\n",
    "* Experimentando con DNC alimentando una imagen a la vez en orden aleatorio con pacientes tambi√©n en orden aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_controller = Controller()\n",
    "dnc_model = DNC(controller=conv_controller,memory_size = (5,5),read_heads=2,device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_criterion = torch.nn.BCELoss()\n",
    "def loss_function(y,y_hat,last_flag):\n",
    "    #print(y,y_hat,last_flag)\n",
    "    #base_criterion = torch.nn.BCELoss()\n",
    "    return torch.full_like(y,last_flag) * base_criterion(y,y_hat)\n",
    "    #return base_criterion(y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = loss_function\n",
    "optimizer = optim.Adam(dnc_model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "source": [
    "\n",
    "total_accuracies  = []\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_predictions = []\n",
    "    epoch_real_values = []\n",
    "    # en cada epoch procesar los pacientes en orden aleatorio\n",
    "    pacientes = np.random.choice(np.array(diagnosticos.paciente),size= len(diagnosticos.paciente),replace=False)\n",
    "    \n",
    "    conteo_pacientes = 0\n",
    "    for paciente in pacientes:\n",
    "        #TODO: remover esta validacion, solo puesta para probar una unica iteracion en compu lenta\n",
    "        if conteo_pacientes >= 99999999:\n",
    "            break\n",
    "            \n",
    "        dnc_model.reset()\n",
    "        read_vectors = torch.zeros(size=(1,dnc_model.R*dnc_model.W)).to(device)\n",
    "        \n",
    "        imagenes_paciente = diccionario_imagenes_pacientes.get(paciente)\n",
    "        diagnostico_hemorragia_paciente = np.array(float(diagnosticos[diagnosticos.paciente==paciente].hemorragia))\n",
    "        tensor_diagnostico_hemorragia_paciente = torch.Tensor(diagnostico_hemorragia_paciente).to(device)\n",
    "        \n",
    "        indices_imagenes_pacientes = np.arange(0,len(imagenes_paciente)-1,step=1)\n",
    "        indices_aleatorios_imagenes = np.random.choice(indices_imagenes_pacientes,len(indices_imagenes_pacientes),replace=False)\n",
    "        \n",
    "        losses = []\n",
    "        for indice in indices_aleatorios_imagenes:\n",
    "            last_image =  int(indice  == indices_aleatorios_imagenes[-1])\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            \n",
    "            imagen_paciente = imagenes_paciente[indice]\n",
    "            \n",
    "            if imagen_paciente.shape != (512,512):\n",
    "                #TODO: tread different image sizes with reshaping, resizing(or other ideas)\n",
    "                continue\n",
    "                \n",
    "            tensor_imagen_paciente =  torch.unsqueeze(\n",
    "                torch.unsqueeze( torch.Tensor(imagen_paciente),dim=0),dim=1).to(device)\n",
    "            \n",
    "            #print(\"Alimentando paciente {} e imagen {} al modelo\".format(paciente,indice),imagen_paciente.shape)\n",
    "            \n",
    "            diagnostico_hemorragia_aproximado,read_vectors = dnc_model(tensor_imagen_paciente,read_vectors)\n",
    "            loss = criterion(diagnostico_hemorragia_aproximado,tensor_diagnostico_hemorragia_paciente,last_image)\n",
    "            \n",
    "            losses.append(loss.view((1,1)))\n",
    "            \n",
    "            if last_image:\n",
    "                y_hat = diagnostico_hemorragia_aproximado.data.cpu().numpy()[0][0]\n",
    "                y_hat_hard = float(y_hat >= 0.5)\n",
    "                epoch_predictions.append(y_hat_hard)\n",
    "                epoch_real_values.append(float(diagnostico_hemorragia_paciente))\n",
    "                \n",
    "                #print(\"--Flag ultima imagen:{} diagnostico:{} valor real{}\".format(last_image,y_hat,diagnostico_hemorragia_paciente))\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                patient_loss = torch.cat(losses).sum()\n",
    "                \n",
    "                patient_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "        conteo_pacientes += 1\n",
    "            \n",
    "    epoch_predictions = np.array(epoch_predictions)\n",
    "    epoch_real_values = np.array(epoch_real_values)\n",
    "    correct_predictions = epoch_predictions == epoch_real_values\n",
    "    accuracy = np.average(correct_predictions)\n",
    "    total_accuracies.append(accuracy)\n",
    "    print(\"Epoch {}: accuracy {}\".format(epoch,accuracy),epoch_predictions,epoch_real_values)\n",
    "\n",
    "print(np.average(total_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controller.conv_controller.conv1.weight\n",
      "controller.conv_controller.conv1.bias\n",
      "controller.conv_controller.fc1.weight\n",
      "controller.conv_controller.fc1.bias\n",
      "controller.fc1.weight\n",
      "controller.fc1.bias\n",
      "controller.fc2.weight\n",
      "controller.fc2.bias\n",
      "output_vector_linear.weight\n",
      "output_vector_linear.bias\n",
      "interface_vector_linear.weight\n",
      "interface_vector_linear.bias\n",
      "read_vectors_to_output_linear.weight\n",
      "read_vectors_to_output_linear.bias\n"
     ]
    }
   ],
   "source": [
    "#TODO: averiguar por que salen 6 tensores de parametros si solo se han declarado 3(al momento de correr lap rueba)\n",
    "train_parmams = list(dnc_model.named_parameters())\n",
    "\n",
    "for train_param in train_parmams:\n",
    "    print(train_param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnc_model.memory_matrix.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta (por detallar))\n",
    "* L temporal link matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM con conv\n",
    "* Experimentando con lstm alimentando una imagen a la vez en orden aleatorio con pacientes tambi√©n en orden aleatorio\n",
    "\n",
    "El vector de entrada de la lstm es un vector producido por una convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVNET_OUTPUT_SIZE = 512 #1000 #1024\n",
    "CONVNET_HIDDEN_SIZE = 512\n",
    "\n",
    "LSTM_HIDDEN_SIZE = 512\n",
    "\n",
    "FINAL_LAYER_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/pytorch_challenge/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "architecture = 'densenet121'\n",
    "#architecture = 'resnet18'\n",
    "architecture_constructor = getattr(models,architecture)\n",
    "model  =  architecture_constructor(pretrained=True)\n",
    "features_size = list(model.children())[-1].in_features\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#freeze parameters so we don't backpropagete  through them\n",
    "layers_to_freeze = 50\n",
    "layer_num = 0\n",
    "for parameter in model.parameters():\n",
    "    if layer_num >= layers_to_freeze:\n",
    "        break\n",
    "    parameter.requires_grad = False\n",
    "        \n",
    "    layer_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier = torch.nn.Sequential(OrderedDict([\n",
    "    (\"fc1\",torch.nn.Linear(features_size,CONVNET_OUTPUT_SIZE))#, #original model only had this\n",
    "    #(\"relu\",torch.nn.ReLU()),\n",
    "    #(\"fc2\",torch.nn.Linear(CONVNET_OUTPUT_SIZE,CONVNET_OUTPUT_SIZE)),\n",
    "    #(\"relu2\",torch.nn.ReLU()),\n",
    "    #(\"fc3\",torch.nn.Linear(CONVNET_OUTPUT_SIZE,CONVNET_OUTPUT_SIZE))\n",
    "]))\n",
    "\n",
    "model.classifier = model_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,conv_net,lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_net = conv_net\n",
    "        self.lstm = nn.LSTM(input_size= CONVNET_OUTPUT_SIZE,hidden_size = LSTM_HIDDEN_SIZE,num_layers=lstm_layers,batch_first = True)\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_hidden_size = LSTM_HIDDEN_SIZE\n",
    "        \n",
    "        self.output_linear = nn.Linear(LSTM_HIDDEN_SIZE,5)\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        x,hidden = self.lstm(x,hidden)\n",
    "        x = x.contiguous().view(-1,self.lstm_hidden_size)\n",
    "        \n",
    "        x = self.output_linear(x)\n",
    "        #x = torch.sigmoid(self.output_linear(x))\n",
    "        \n",
    "        return x,hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weigths =  next(self.lstm.parameters())\n",
    "        \n",
    "        \n",
    "        hidden = ( \n",
    "            weigths.new(self.lstm_layers,1,LSTM_HIDDEN_SIZE).zero_().to(device)\n",
    "        ,   weigths.new(self.lstm_layers,1,LSTM_HIDDEN_SIZE).zero_().to(device)\n",
    "                 )\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ya que estamos usando densenet121 transformamos los datos de entrada para que tengan el tama√±o adecuado\n",
    "# y se normalicen usando los valores de media y desviaci√≥n estandar del dataset usado en densenet\n",
    "train_data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lstm = ConvLSTM(model,lstm_layers=1)\n",
    "conv_lstm.to(device)\n",
    "\n",
    "base_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.00030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracies  = []\n",
    "val_total_accuracies  = []\n",
    "total_f1_accuracies = []\n",
    "val_total_f1_accuracies = []\n",
    "total_losses = []\n",
    "individual_accuracies = []\n",
    "val_individual_accuracies = []\n",
    "individual_f1_accuracies = []\n",
    "val_individual_f1_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_precisions = []\n",
    "val_individual_precisions = []\n",
    "individual_recalls = []\n",
    "val_individual_recalls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.000010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_pred,y_real):\n",
    "    \"for calculating the accurracy of multiple columns\"\n",
    "    assert y_pred.shape[1] == y_real.shape[1]\n",
    "    \n",
    "    num_columns = y_pred.shape[1]\n",
    "    \n",
    "    accuracies = []\n",
    "    for i in range(num_columns):\n",
    "        \n",
    "        colum_acc = accuracy_score(y_real[:,i],y_pred[:,i])\n",
    "        accuracies.append(colum_acc)\n",
    "        \n",
    "    return accuracies, np.mean(np.array(accuracies))\n",
    "\n",
    "def calc_f1_score(y_pred,y_real):\n",
    "    \"for calculating the f1 acc score of multiple columns\"\n",
    "    assert y_pred.shape[1] == y_real.shape[1]\n",
    "    \n",
    "    num_columns = y_pred.shape[1]\n",
    "    \n",
    "    accuracies = []\n",
    "    for i in range(num_columns):\n",
    "        \n",
    "        colum_acc = f1_score(y_real[:,i],y_pred[:,i])\n",
    "        accuracies.append(colum_acc)\n",
    "        \n",
    "    return accuracies, np.mean(np.array(accuracies))\n",
    "\n",
    "def calc_score(y_pred,y_real,score_func):\n",
    "    \"for calculating the f1 acc score of multiple columns\"\n",
    "    assert y_pred.shape[1] == y_real.shape[1]\n",
    "    \n",
    "    num_columns = y_pred.shape[1]\n",
    "    \n",
    "    accuracies = []\n",
    "    for i in range(num_columns):\n",
    "        \n",
    "        colum_acc = score_func(y_real[:,i],y_pred[:,i])\n",
    "        accuracies.append(colum_acc)\n",
    "        \n",
    "    return accuracies, np.mean(np.array(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lstm.load_state_dict(torch.load(\"./model4.chpk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/pytorch_challenge/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/luis/anaconda2/envs/pytorch_challenge/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:individual accs:[[0.952 0.897 0.927 0.981 0.846]] avg accuracy 0.921 loss:0.237 avg val acc:0.875 time 574.0580441951752\n",
      "Epoch 1:individual accs:[[0.958 0.897 0.927 0.981 0.846]] avg accuracy 0.922 loss:0.236 avg val acc:0.875 time 573.6806719303131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "conv_lstm.train()\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    iteration_number = 0\n",
    "    epoch_start  = time.time()\n",
    "    conv_lstm.train()\n",
    "    \n",
    "    epoch_predictions = []\n",
    "    epoch_real_values = []\n",
    "    val_real_values = []\n",
    "    val_predictions = []\n",
    "    epoch_losses = []\n",
    "    # en cada epoch procesar los pacientes en orden aleatorio\n",
    "    pacientes = np.random.choice(np.array(diagnosticos.paciente),size= len(diagnosticos.paciente),replace=False)\n",
    "    \n",
    "    conteo_pacientes = 0\n",
    "    for paciente in pacientes:\n",
    "        \n",
    "            \n",
    "        h = conv_lstm.init_hidden()\n",
    "        \n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        #conv_lstm.zero_grad()\n",
    "        \n",
    "        imagenes_paciente = diccionario_imagenes_pacientes.get(paciente)\n",
    "        diagnostico_hemorragia_paciente = np.array(float(diagnosticos[diagnosticos.paciente==paciente].hemorragia))\n",
    "        vector_diagnostico_paciente = np.array(diagnosticos[diagnosticos.paciente==paciente][[\"hemorragia\",\"isquemia\",\"fractura\",\"masa\",\"edema\"]])\n",
    "        tensor_diagnostico_paciente = torch.Tensor(vector_diagnostico_paciente).view((1,5)).to(device)\n",
    "        \n",
    "        tensor_diagnostico_hemorragia_paciente = torch.Tensor(diagnostico_hemorragia_paciente).view((1,1)).to(device)\n",
    "        \n",
    "        indices_imagenes_pacientes = np.arange(0,len(imagenes_paciente)-1,step=1)\n",
    "        indices_aleatorios_imagenes = np.random.choice(indices_imagenes_pacientes,len(indices_imagenes_pacientes),replace=False)\n",
    "        \n",
    "        losses = []\n",
    "        for indice in indices_aleatorios_imagenes:\n",
    "            #h = tuple([each.data for each in h])\n",
    "            #print(paciente,indice)\n",
    "            last_image =  int(indice  == indices_aleatorios_imagenes[-1])\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            \n",
    "            imagen_paciente =  np.expand_dims(imagenes_paciente[indice],2)\n",
    "            imagen_paciente =  np.repeat(imagen_paciente,3,axis=2)\n",
    "            #plt.imshow(imagen_paciente)\n",
    "            #plt.show()\n",
    "            \n",
    "            tensor_imagen_paciente =  train_data_transforms(imagen_paciente).unsqueeze(0).to(device)\n",
    "            #plt.imshow(np.squeeze(tensor_imagen_paciente.permute(2,3,1,0).cpu().numpy()))\n",
    "            #plt.show()\n",
    "            #print(\"printed it\")\n",
    "            #print(\"Alimentando paciente {} e imagen {} al modelo\".format(paciente,indice),imagen_paciente.shape)\n",
    "            \n",
    "            diagnostico_aproximado,h  = conv_lstm(tensor_imagen_paciente,h)\n",
    "            prob_diagnostico_aproximado = torch.sigmoid(diagnostico_aproximado)\n",
    "            diagnostico_hemorragia_aproximado = diagnostico_aproximado[:,0]\n",
    "            \n",
    "            \n",
    "            #loss = base_criterion(diagnostico_hemorragia_aproximado,tensor_diagnostico_hemorragia_paciente)\n",
    "            \n",
    "            #losses.append(loss.view((1,1)))\n",
    "            \n",
    "            if last_image:\n",
    "                \n",
    "                loss =  base_criterion(diagnostico_aproximado,tensor_diagnostico_paciente)\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(conv_lstm.lstm.parameters(), 5.0)\n",
    "                #optimizer.step()\n",
    "                \n",
    "                vector_y_hat_hard = prob_diagnostico_aproximado >= 0.5\n",
    "                \n",
    "                #print(torch.sigmoid(diagnostico_aproximado).data,tensor_diagnostico_paciente.data)\n",
    "                y_hat = diagnostico_hemorragia_aproximado.data.cpu().numpy()[0]\n",
    "                y_hat_hard = float(y_hat >= 0.5)\n",
    "                \n",
    "                epoch_predictions.append(vector_y_hat_hard.data.cpu().numpy()[0])\n",
    "                epoch_real_values.append(vector_diagnostico_paciente[0])\n",
    "                \n",
    "                \n",
    "                #print(\"--Flag ultima imagen:{} diagnostico:{} valor real{}\".format(last_image,y_hat,diagnostico_hemorragia_paciente))\n",
    "                #optimizer.zero_grad()\n",
    "                \n",
    "                #patient_loss = torch.cat(losses).mean()\n",
    "                \n",
    "                \n",
    "                #patient_loss.backward()\n",
    "                #loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(conv_lstm.lstm.parameters(), 5.0)\n",
    "                #optimizer.step()\n",
    "                \n",
    "                epoch_losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "                \n",
    "        conteo_pacientes += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    #epoch_predictions = np.array(epoch_predictions)\n",
    "    #epoch_real_values = np.array(epoch_real_values)\n",
    "    #correct_predictions = epoch_predictions == epoch_real_values\n",
    "    #accuracy = np.average(correct_predictions)\n",
    "        if iteration_number % MINI_BATCH_SIZE == 0 or iteration_number == len(pacientes)-9:\n",
    "            nn.utils.clip_grad_norm_(conv_lstm.lstm.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            conv_lstm.zero_grad()\n",
    "            \n",
    "            \n",
    "        \n",
    "        iteration_number += 1\n",
    "    accuracies,average_accuracy = calc_score(np.array(epoch_predictions),np.array(epoch_real_values),accuracy_score)\n",
    "    precision_accs,average_prec_acc = calc_score(np.array(epoch_predictions),np.array(epoch_real_values),precision_score)\n",
    "    recall_accs,average_recall_acc = calc_score(np.array(epoch_predictions),np.array(epoch_real_values),recall_score)\n",
    "    f1_accs,average_f1_acc = calc_score(np.array(epoch_predictions),np.array(epoch_real_values),f1_score)\n",
    "    \n",
    "    \n",
    "    epoch_avg_loss = np.average(epoch_losses)\n",
    "    total_losses.append(epoch_avg_loss)\n",
    "    \n",
    "    total_accuracies.append(average_accuracy)\n",
    "    individual_accuracies.append(accuracies)\n",
    "    individual_precisions.append(precision_accs)\n",
    "    individual_recalls.append(recall_accs)\n",
    "    total_f1_accuracies.append(average_f1_acc)\n",
    "    individual_f1_accuracies.append(f1_accs)\n",
    "    \n",
    "    conv_lstm.eval()\n",
    "    for paciente in diagonsticos_val.paciente:\n",
    "        imagenes_paciente = diccionario_imagenes_pacientes_val[paciente]\n",
    "        \n",
    "            \n",
    "        h = conv_lstm.init_hidden()\n",
    "        \n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        #conv_lstm.zero_grad()\n",
    "        \n",
    "        for indice in range(len(imagenes_paciente)):\n",
    "            imagen_paciente =  np.expand_dims(imagenes_paciente[indice],2)\n",
    "            imagen_paciente =  np.repeat(imagen_paciente,3,axis=2)\n",
    "               \n",
    "            tensor_imagen_paciente =  val_data_transforms(imagen_paciente).unsqueeze(0).to(device)\n",
    "            \n",
    "            \n",
    "            #print(\"Alimentando paciente {} e imagen {} al modelo\".format(paciente,indice),imagen_paciente.shape)\n",
    "            \n",
    "            diagnostico_aproximado,h  = conv_lstm(tensor_imagen_paciente,h)\n",
    "            prob_diagnostico_aproximado = torch.sigmoid(diagnostico_aproximado).detach()\n",
    "            \n",
    "            vector_y_hat_hard = prob_diagnostico_aproximado >= 0.5\n",
    "            \n",
    "            if indice == len(imagenes_paciente)-1:\n",
    "                val_predictions.append(vector_y_hat_hard.data.cpu().numpy()[0])\n",
    "            #print(diagonsticos_val[diagonsticos_val.paciente==paciente],vector_y_hat_hard)\n",
    "    \n",
    "        vector_diagnostico_paciente = np.array(diagonsticos_val[diagonsticos_val.paciente==paciente][[\"hemorragia\",\"isquemia\",\"fractura\",\"masa\",\"edema\"]])\n",
    "        val_real_values.append(vector_diagnostico_paciente[0])\n",
    "      \n",
    "    \n",
    "    val_accuracies,val_average_accuracy = calc_score(np.array(val_predictions),np.array(val_real_values),accuracy_score)\n",
    "    val_precision_accs,val_average_prec_acc = calc_score(np.array(val_predictions),np.array(val_real_values),precision_score)\n",
    "    val_recall_accs,val_average_recall_acc = calc_score(np.array(epoch_predictions),np.array(epoch_real_values),recall_score)\n",
    "    val_f1_accs,val_average_f1_acc = calc_score(np.array(epoch_predictions),np.array(epoch_real_values),f1_score)\n",
    "    \n",
    "    val_total_accuracies.append(val_average_accuracy)\n",
    "    val_individual_accuracies.append(val_accuracies)\n",
    "    val_individual_precisions.append(val_precision_accs)\n",
    "    val_individual_recalls.append(val_recall_accs)\n",
    "    val_individual_f1_accuracies.append(val_f1_accs)\n",
    "    \n",
    "    torch.save(conv_lstm.state_dict(), \"./model4.chpk\")\n",
    "    epoch_end  = time.time()\n",
    "    print(\"Epoch {}:individual accs:[{}] avg accuracy {:.3f} loss:{:.3f} avg val acc:{:.3f} time {}\".format(epoch,\n",
    "                                                                                                        np.around(accuracies,decimals=3),\n",
    "                                                                                                        average_accuracy,epoch_avg_loss,\n",
    "                                                                                                        val_average_accuracy,epoch_end-epoch_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(conv_lstm.state_dict(), \"./model4.chpk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_accuracies_tensor = np.asarray(individual_accuracies)\n",
    "val_individual_accuracies_tensor = np.asarray(val_individual_accuracies)\n",
    "individual_precisions_tensor = np.asarray(individual_precisions)\n",
    "individual_recalls_tensor = np.asarray(individual_recalls)\n",
    "individual_f1_accuracies_tensor = np.asarray(individual_f1_accuracies)\n",
    "val_individual_precisions_tensor = np.asarray(val_individual_precisions)\n",
    "val_individual_recalls_tensor = np.asarray(val_individual_recalls)\n",
    "val_individual_f1_accuracies_tensor = np.asarray(val_individual_f1_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training average accuracy\n",
    "\n",
    "Accuracies of al diagnostics averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val average accuracy\n",
    "\n",
    "Accuracies of al diagnostics averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_total_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual accuracies plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_plots = individual_accuracies_tensor.shape[1]  \n",
    "f,plots = plt.subplots(number_of_plots,4,sharex=True,figsize= (12,25))\n",
    "print(\"train plots\")\n",
    "for i in range(number_of_plots):\n",
    "    plots[i,0].plot(individual_accuracies_tensor[:,i])\n",
    "    plots[i,0].set_title(nombres_diagnosticos[i]+ \" acc\")\n",
    "    \n",
    "    plots[i,1].plot(individual_precisions_tensor[:,i])\n",
    "    plots[i,1].set_title(nombres_diagnosticos[i]+\" precision\")\n",
    "    \n",
    "    plots[i,2].plot(individual_recalls_tensor[:,i])\n",
    "    plots[i,2].set_title(nombres_diagnosticos[i]+\" recall\")\n",
    "    \n",
    "    plots[i,3].plot(individual_f1_accuracies_tensor[:,i])\n",
    "    plots[i,3].set_title(nombres_diagnosticos[i]+\" f1-score\")\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "print(\"validation plots\")\n",
    "number_of_plots =  individual_accuracies_tensor.shape[1]  \n",
    "f,plots = plt.subplots(number_of_plots,4,sharex=True,figsize= (12,25))\n",
    "\n",
    "for i in range(number_of_plots):\n",
    "    plots[i,0].plot(val_individual_accuracies_tensor[:,i])\n",
    "    plots[i,0].set_title(nombres_diagnosticos[i]+ \" val acc\")\n",
    "    \n",
    "    plots[i,1].plot(val_individual_precisions_tensor[:,i])\n",
    "    plots[i,1].set_title(nombres_diagnosticos[i]+\" val precision\")\n",
    "    \n",
    "    plots[i,2].plot(val_individual_recalls_tensor[:,i])\n",
    "    plots[i,2].set_title(nombres_diagnosticos[i]+\" val recall\")\n",
    "    \n",
    "    plots[i,3].plot(val_individual_f1_accuracies_tensor[:,i])\n",
    "    plots[i,3].set_title(nombres_diagnosticos[i]+\" val f1-score\")\n",
    "    \n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "* El mejor resultado hasta ahora ha sido no congelar pesos(entranar toda la convnet densenet121) agregandole una sola capa fully connected de salida, y 3 layers en la lstm, todas las capas con 1024 de tama√±o. Lr = 0.001.\n",
    "* Congelando las primeras 50 capas de la convnet converge alrededor de los 40 epochs(pero sigue bajando) con la misma configuraci√≥n qeu el resultado 1.\n",
    "* Misma arquitectura pero congelando 100 capas de la convnet(y agregando una nueva muestra de pacientes de 3 ) converge alrededor de los 25 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "* Normalizar  el allocation weighitng con sofmax(en la primera iteraci√≥n asigna todo el peso a la primera posici√≥n de memoria)\n",
    "* Usar arquitectura similar a dueling network o inception para tener 2 caminos en las entradas.\n",
    "* Cambiar el modelo original para leer antes que escribir y usar lo leido para sacar una predicci√≥n en ese punto en el tiempo(el modelo original lee de la memoria despues de escribir y usa la info leida en el siguiente paso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_individual_precisions_tensor[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_individual_f1_accuracies_tensor[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2*(val_individual_precisions_tensor[:,0]*val_individual_recalls_tensor[:,0])/((val_individual_precisions_tensor[:,0]+val_individual_recalls_tensor[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

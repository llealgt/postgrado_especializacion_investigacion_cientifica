{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.15</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Warning: You are not running the latest version of PixieDust. Current is 1.1.15, Latest is 1.1.17</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>Please copy and run the following command in a new cell to upgrade: <span style=\"background-color:#ececec;font-family:monospace;padding:0 5px\">!pip install --user --upgrade pixiedust</span></div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Please restart kernel after upgrading.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pixiedust\n",
    "from torchvision import transforms,datasets,models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../datos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosticos  = pd.read_excel(DATA_DIRECTORY+\"RESUMEN TAC CEREBRALES.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diccionario cuya llave es el id de paciente y el valor una lista \n",
    "# donde cada elemento de la lista es la matriz de una i\n",
    "diccionario_imagenes_pacientes = dict()\n",
    "\n",
    "for paciente in diagnosticos.paciente:\n",
    "    directorio_paciente = DATA_DIRECTORY+\"paciente_\"+str(paciente)\n",
    "    archivos_paciente = os.listdir(directorio_paciente)\n",
    "    \n",
    "    lista_imagenes_paciente = []\n",
    "    for archivo in archivos_paciente:\n",
    "        if archivo.endswith(\".jpg\"):\n",
    "            imagen = mpimg.imread(directorio_paciente+\"/\"+archivo)\n",
    "            lista_imagenes_paciente.append(imagen)\n",
    "            \n",
    "    diccionario_imagenes_pacientes[paciente] = lista_imagenes_paciente\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos y arquitecturas\n",
    "### Arquitecturas experimental  DNC\n",
    "* Alimentamos al modelo imagen por imagen y se presenta un solo diagnostico por paciente\n",
    "* El controller de la DNC esta compuesto por una convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_OUTPUT_SIZE = 128\n",
    "READ_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: cambiar valores quemados por valores parametrizados y calculos dependientes\n",
    "class ConvController(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,4,kernel_size=3,stride=1)\n",
    "        self.fc1  =  torch.nn.Linear(262144,CONTROLLER_OUTPUT_SIZE)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        h = self.conv1(x)\n",
    "        \n",
    "        #flatten\n",
    "        h =  x.view(-1,x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        h =  self.fc1(h)\n",
    "        \n",
    "        return h #h_t in my txt\n",
    "    \n",
    "class Controller(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_controller = ConvController()\n",
    "        self.fc1 = torch.nn.Linear(10,CONTROLLER_OUTPUT_SIZE)\n",
    "        self.fc2 = torch.nn.Linear(2*CONTROLLER_OUTPUT_SIZE,CONTROLLER_OUTPUT_SIZE)\n",
    "        \n",
    "    def forward(self,x,read_vectors):\n",
    "        h_conv = self.conv_controller(x)\n",
    "        h_read_vectors = self.fc1(read_vectors)\n",
    "        \n",
    "        h_t = torch.cat((h_conv,h_read_vectors),dim=1)\n",
    "        \n",
    "        h_t =  torch.relu( h_t)\n",
    "        h_t =  self.fc2(h_t) \n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: cambiar valores quemados por valores parametrizados y calculos dependientes\n",
    "#TODO: cordar por que en algun momento le puse bias = False a los pesos del vector de salida de la DNC\n",
    "\n",
    "\n",
    "class DNC(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,controller,memory_size = (10,10),read_heads = 1,device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.controller = controller\n",
    "        self.device = device\n",
    "        self.N = memory_size[0] # number of memory locations\n",
    "        self.W = memory_size[1] # word size of the memory \n",
    "        self.R = read_heads # number of read heads\n",
    "        self.WS = 1 #not in the paper(they use 1), but used as a parametrizable number of write heads for further experiments\n",
    "        self.interface_vector_size = (self.W*self.R) + (self.W*self.WS) + (2*self.W) + (5*self.R) + 3\n",
    "        \n",
    "        # inicialization st to random just for testing, remember to put on zeros\n",
    "        #self.memory_matrix = self.memory_matrix =  nn.Parameter(torch.zeros(size=memory_size),requires_grad= False) \n",
    "        \n",
    "        #1024 es el tamaño del vector de salida del controlador, 1 es el tamaño de salida de la dnc\n",
    "        self.output_vector_linear = torch.nn.Linear(CONTROLLER_OUTPUT_SIZE,1,bias=True) #W_y \n",
    "        self.interface_vector_linear = torch.nn.Linear(CONTROLLER_OUTPUT_SIZE,self.interface_vector_size,bias=True) #W_ξ\n",
    "        self.read_vectors_to_output_linear = torch.nn.Linear(self.R*self.W,1,bias = True) #W_r in my txt\n",
    "        \n",
    "        self.read_keys = torch.Tensor(size=(self.R,self.W)).requires_grad_(False) # k_r in my txt\n",
    "        self.read_strenghts = torch.Tensor(size=(self.R,1)).requires_grad_(False) #β_r\n",
    "        \n",
    "        #self.read_weighting = torch.Tensor(torch.zeros(size=(self.R,self.N))).requires_grad_(False).to(device) #r_w\n",
    "        \n",
    "        self.write_key = torch.Tensor(size=(1,self.W)).requires_grad_(False) # k_w in my txt\n",
    "        self.write_strenght = torch.Tensor(size=(1,1)).requires_grad_(False) # β_w\n",
    "        \n",
    "        #self.write_weighting = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(False) # w_w\n",
    "        \n",
    "        #self.usage_vector = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(False) #u_t\n",
    "        \n",
    "        self.memory_matrix_ones = torch.Tensor(torch.ones(size=memory_size)).requires_grad_(True).to(device) #E on paper\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def forward(self,x,read_vectors):\n",
    "        \n",
    "        h_t = self.controller(x,read_vectors) #controller output called ht in the paper\n",
    "        \n",
    "        output_vector = self.output_vector_linear(h_t) # called Vt in the paper(υ=Wy[h1;...;hL]) v_o_t in my txt\n",
    "        interface_vector = self.interface_vector_linear(h_t).data #called ξt(ksi) in the paper ,ξ_t in my txt\n",
    "        \n",
    "        self.read_keys.data = interface_vector[0,0:self.R*self.W].view((self.R,self.W)) #k_r in my txt\n",
    "        \n",
    "        #clamp temporary added because the exp was returning inf  values\n",
    "        read_strenghts =  torch.clamp( interface_vector[0,self.R*self.W:self.R*self.W+self.R].view((self.R,1)),max=85)\n",
    "        self.read_strenghts.data = self.oneplus(read_strenghts) #β_r\n",
    "        \n",
    "        self.write_key.data = interface_vector[0,self.R*self.W+self.R:self.R*self.W+self.R+self.W].view((1,self.W)) # k_w\n",
    "        \n",
    "        write_strenght = torch.clamp(interface_vector[:,self.R*self.W+self.R+self.W:self.R*self.W+self.R+self.W + 1].view((1,1)),max=85)\n",
    "        self.write_strenght.data = self.oneplus(write_strenght) #β_w\n",
    "        \n",
    "        erase_vector = interface_vector[0,self.R*self.W+self.R+self.W + 1: self.R*self.W+self.R+self.W + 1 + self.W].view((1,self.W))\n",
    "        erase_vector = torch.sigmoid(erase_vector) #e_t\n",
    "        \n",
    "        write_vector = interface_vector[0,self.R*self.W+self.R+self.W + 1 + self.W:self.R*self.W+self.R+self.W + 1 + 2*self.W].view((1,self.W)) #v_t\n",
    "        \n",
    "        free_gates  =  interface_vector[0,self.R*self.W+self.R+self.W + 1 + 2*self.W:self.R*self.W+2*self.R+self.W + 1 + 2*self.W].view((self.R,1)) #f_t\n",
    "        free_gates =   torch.sigmoid(free_gates)\n",
    "        \n",
    "        allocation_gate = interface_vector[0,self.R*self.W+2*self.R+self.W + 1 + 2*self.W:self.R*self.W+2*self.R+self.W + 1 + 2*self.W+1]\n",
    "        allocation_gate = torch.sigmoid(allocation_gate)\n",
    "        \n",
    "        write_gate = interface_vector[0,self.R*self.W+2*self.R+self.W + 1 + 2*self.W+1:self.R*self.W+2*self.R+self.W + 1 + 2*self.W+2]\n",
    "        write_gate = torch.sigmoid( write_gate)\n",
    "        \n",
    "        \n",
    "        # Escritura\n",
    "        # TODO: verificar y/o experimentar si el ordern es :primero escribir y luego leer de la memoria(asi parece en el pazper)\n",
    "        retention_vector = (1.0 - free_gates * self.read_weighting).prod(dim=0)\n",
    "        self.usage_vector.data = (self.usage_vector +self.write_weighting - (self.usage_vector *self.write_weighting))*retention_vector #u_t\n",
    "        allocation_weighting = self.calc_allocation_weighting(self.usage_vector)\n",
    "        write_content_weighting = self.content_lookup(self.memory_matrix,self.write_key,self.write_strenght)\n",
    "\n",
    "        self.write_weighting.data =  write_gate*(  \n",
    "            (allocation_gate * allocation_weighting) +  ((1- allocation_gate)*write_content_weighting))\n",
    "        \n",
    "        new_memory_matrix = self.memory_matrix*(self.memory_matrix_ones - torch.matmul(self.write_weighting.t(),erase_vector)) + torch.matmul(self.write_weighting.t(),write_vector)\n",
    "        \n",
    "        self.memory_matrix.data = new_memory_matrix\n",
    "        \n",
    "        # read by content weithing(attention by similarity)\n",
    "        read_content_weighting = self.content_lookup(self.memory_matrix,self.read_keys,self.read_strenghts)\n",
    "        \n",
    "        #read weithing is a combination of reading modes,TODO:add temporal attention not just by similarity\n",
    "        self.read_weighting.data = read_content_weighting\n",
    "        \n",
    "        read_vectors = torch.matmul(self.read_weighting,self.memory_matrix).view((1,self.R*self.W)) #r in my txt\n",
    "        read_heads_to_output = self.read_vectors_to_output_linear(read_vectors) #v_r_t in my t xt\n",
    "        \n",
    "        #TODO: experiment and decide if maintain sigmoid\n",
    "        y_t = torch.sigmoid(output_vector + read_heads_to_output)\n",
    "        return y_t,read_vectors\n",
    "    \n",
    "    def oneplus(self,x):\n",
    "        # apply oneplus operation to a tensor to constrain it's elements to [1,inf)\n",
    "        #TODO: check numerical statiliby as exp is returning inf for numbers like 710,emporary added clamp to 85\n",
    "        return torch.log(1+torch.exp(x)) + 1\n",
    "    \n",
    "    def content_lookup(self,matrix,keys,strengths):\n",
    "        # returns a probability distribution over the memory locations \n",
    "        # with higher probability to memory locations with bigger similarity to the keys\n",
    "        # bigger strenght make more aggresive distributions ,for example a distribution (0.2,0.3,0.5) with\n",
    "        # bigger strenght becomes (0.1,0.12,0.78)\n",
    "        # returns tensor of shape (read keys,memory size) = (R,N)\n",
    "        keys_norm =  torch.sqrt(torch.sum(keys**2,dim=1).unsqueeze(dim=1))\n",
    "        matrix_norm = torch.sqrt(torch.sum(matrix**2,dim=1))\n",
    "        norms_multiplication = keys_norm*matrix_norm\n",
    "        # calc cosine similarity between keys and memory locations(1e-6 is used avoiding div by 0)\n",
    "        divide_zero_prevent_factor = torch.zeros_like(norms_multiplication).add_(1e-6)\n",
    "        cosine_similarity = torch.matmul(keys,matrix.t())/(torch.max(norms_multiplication,divide_zero_prevent_factor))\n",
    "        \n",
    "        # do a \"strenght\" softmax to calculate the probability distribution\n",
    "        numerator = torch.exp(cosine_similarity*strengths)\n",
    "        denominator = numerator.sum(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "        distribution = numerator/denominator\n",
    "        \n",
    "        return distribution\n",
    "    \n",
    "    def calc_allocation_weighting(self,usage_vector):\n",
    "        #print(\"usage vector\",usage_vector)\n",
    "        _,free_list = torch.topk(-usage_vector,self.N,dim=1) #φt indices of memory locations ordered by usage\n",
    "        #print(\"free list\",free_list)\n",
    "        free_list = free_list.view(-1)\n",
    "        #print(\"reshaped free list\",free_list)\n",
    "        _,ordered_free_list =  torch.topk(-free_list,self.N)\n",
    "        ordered_free_list = ordered_free_list.view(-1)\n",
    "        #print(\"ordered free list\",ordered_free_list)\n",
    "        ordered_usage_vector = usage_vector[:,free_list]\n",
    "        #print(\"ordered usage vector\",ordered_usage_vector)\n",
    "        ordered_usage_vector_cumulative_product = torch.ones(size=(1,self.N+1)).to(device)\n",
    "        #print(ordered_usage_vector_cumulative_product)\n",
    "        #print(\"cumprod \",ordered_usage_vector.cumprod(dim=1))\n",
    "        ordered_usage_vector_cumulative_product[0,1:] = ordered_usage_vector.cumprod(dim=1)\n",
    "        #print(ordered_usage_vector_cumulative_product)\n",
    "        \n",
    "        allocation_weighting = (1 - usage_vector)*ordered_usage_vector_cumulative_product[0,ordered_free_list]\n",
    "        \n",
    "        return  allocation_weighting\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory_matrix =  torch.Tensor(torch.zeros(size=(self.N,self.W))).requires_grad_(True).to(device) \n",
    "        self.read_weighting = torch.Tensor(torch.zeros(size=(self.R,self.N))).requires_grad_(True).to(device) #r_w\n",
    "        self.write_weighting = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(True).to(device) # w_w\n",
    "        self.usage_vector = torch.Tensor(torch.zeros(size=(1,self.N))).requires_grad_(True).to(device) #u_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos\n",
    "* Experimentando con DNC alimentando una imagen a la vez en orden aleatorio con pacientes también en orden aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_controller = Controller()\n",
    "dnc_model = DNC(controller=conv_controller,memory_size = (5,5),read_heads=2,device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_criterion = torch.nn.BCELoss()\n",
    "def loss_function(y,y_hat,last_flag):\n",
    "    #print(y,y_hat,last_flag)\n",
    "    #base_criterion = torch.nn.BCELoss()\n",
    "    return torch.full_like(y,last_flag) * base_criterion(y,y_hat)\n",
    "    #return base_criterion(y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = loss_function\n",
    "optimizer = optim.Adam(dnc_model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "source": [
    "\n",
    "total_accuracies  = []\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_predictions = []\n",
    "    epoch_real_values = []\n",
    "    # en cada epoch procesar los pacientes en orden aleatorio\n",
    "    pacientes = np.random.choice(np.array(diagnosticos.paciente),size= len(diagnosticos.paciente),replace=False)\n",
    "    \n",
    "    conteo_pacientes = 0\n",
    "    for paciente in pacientes:\n",
    "        #TODO: remover esta validacion, solo puesta para probar una unica iteracion en compu lenta\n",
    "        if conteo_pacientes >= 99999999:\n",
    "            break\n",
    "            \n",
    "        dnc_model.reset()\n",
    "        read_vectors = torch.zeros(size=(1,dnc_model.R*dnc_model.W)).to(device)\n",
    "        \n",
    "        imagenes_paciente = diccionario_imagenes_pacientes.get(paciente)\n",
    "        diagnostico_hemorragia_paciente = np.array(float(diagnosticos[diagnosticos.paciente==paciente].hemorragia))\n",
    "        tensor_diagnostico_hemorragia_paciente = torch.Tensor(diagnostico_hemorragia_paciente).to(device)\n",
    "        \n",
    "        indices_imagenes_pacientes = np.arange(0,len(imagenes_paciente)-1,step=1)\n",
    "        indices_aleatorios_imagenes = np.random.choice(indices_imagenes_pacientes,len(indices_imagenes_pacientes),replace=False)\n",
    "        \n",
    "        losses = []\n",
    "        for indice in indices_aleatorios_imagenes:\n",
    "            last_image =  int(indice  == indices_aleatorios_imagenes[-1])\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            \n",
    "            imagen_paciente = imagenes_paciente[indice]\n",
    "            \n",
    "            if imagen_paciente.shape != (512,512):\n",
    "                #TODO: tread different image sizes with reshaping, resizing(or other ideas)\n",
    "                continue\n",
    "                \n",
    "            tensor_imagen_paciente =  torch.unsqueeze(\n",
    "                torch.unsqueeze( torch.Tensor(imagen_paciente),dim=0),dim=1).to(device)\n",
    "            \n",
    "            #print(\"Alimentando paciente {} e imagen {} al modelo\".format(paciente,indice),imagen_paciente.shape)\n",
    "            \n",
    "            diagnostico_hemorragia_aproximado,read_vectors = dnc_model(tensor_imagen_paciente,read_vectors)\n",
    "            loss = criterion(diagnostico_hemorragia_aproximado,tensor_diagnostico_hemorragia_paciente,last_image)\n",
    "            \n",
    "            losses.append(loss.view((1,1)))\n",
    "            \n",
    "            if last_image:\n",
    "                y_hat = diagnostico_hemorragia_aproximado.data.cpu().numpy()[0][0]\n",
    "                y_hat_hard = float(y_hat >= 0.5)\n",
    "                epoch_predictions.append(y_hat_hard)\n",
    "                epoch_real_values.append(float(diagnostico_hemorragia_paciente))\n",
    "                \n",
    "                #print(\"--Flag ultima imagen:{} diagnostico:{} valor real{}\".format(last_image,y_hat,diagnostico_hemorragia_paciente))\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                patient_loss = torch.cat(losses).sum()\n",
    "                \n",
    "                patient_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                \n",
    "        conteo_pacientes += 1\n",
    "            \n",
    "    epoch_predictions = np.array(epoch_predictions)\n",
    "    epoch_real_values = np.array(epoch_real_values)\n",
    "    correct_predictions = epoch_predictions == epoch_real_values\n",
    "    accuracy = np.average(correct_predictions)\n",
    "    total_accuracies.append(accuracy)\n",
    "    print(\"Epoch {}: accuracy {}\".format(epoch,accuracy),epoch_predictions,epoch_real_values)\n",
    "\n",
    "print(np.average(total_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controller.conv_controller.conv1.weight\n",
      "controller.conv_controller.conv1.bias\n",
      "controller.conv_controller.fc1.weight\n",
      "controller.conv_controller.fc1.bias\n",
      "controller.fc1.weight\n",
      "controller.fc1.bias\n",
      "controller.fc2.weight\n",
      "controller.fc2.bias\n",
      "output_vector_linear.weight\n",
      "output_vector_linear.bias\n",
      "interface_vector_linear.weight\n",
      "interface_vector_linear.bias\n",
      "read_vectors_to_output_linear.weight\n",
      "read_vectors_to_output_linear.bias\n"
     ]
    }
   ],
   "source": [
    "#TODO: averiguar por que salen 6 tensores de parametros si solo se han declarado 3(al momento de correr lap rueba)\n",
    "train_parmams = list(dnc_model.named_parameters())\n",
    "\n",
    "for train_param in train_parmams:\n",
    "    print(train_param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnc_model.memory_matrix.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta (por detallar))\n",
    "* L temporal link matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM con conv\n",
    "* Experimentando con lstm alimentando una imagen a la vez en orden aleatorio con pacientes también en orden aleatorio\n",
    "\n",
    "El vector de entrada de la lstm es un vector producido por una convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVNET_OUTPUT_SIZE = 64\n",
    "CONVNET_HIDDEN_SIZE = 64\n",
    "\n",
    "LSTM_HIDDEN_SIZE = 64\n",
    "\n",
    "FINAL_LAYER_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/pytorch_challenge/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "architecture = 'densenet121'\n",
    "architecture_constructor = getattr(models,architecture)\n",
    "model  =  architecture_constructor(pretrained=True)\n",
    "features_size = list(model.children())[-1].in_features\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#freeze parameters so we don't backpropagete  through them\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier = torch.nn.Sequential(OrderedDict([\n",
    "    (\"fc1\",torch.nn.Linear(features_size,CONVNET_HIDDEN_SIZE)),\n",
    "    (\"relu\",torch.nn.ReLU()),\n",
    "    (\"fc2\",torch.nn.Linear(CONVNET_HIDDEN_SIZE,CONVNET_OUTPUT_SIZE))\n",
    "]))\n",
    "\n",
    "model.classifier = model_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,conv_net,lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_net = conv_net\n",
    "        self.lstm = nn.LSTM(input_size= CONVNET_OUTPUT_SIZE,hidden_size = LSTM_HIDDEN_SIZE,num_layers=lstm_layers,batch_first = True)\n",
    "        self.lstm_layers = lstm_layers\n",
    "        \n",
    "        self.output_linear = nn.Linear(LSTM_HIDDEN_SIZE,1)\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        x,hidden = self.lstm(x,hidden)\n",
    "        x = torch.sigmoid(self.output_linear(x))\n",
    "        \n",
    "        return x,hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weigths =  next(self.lstm.parameters())\n",
    "        \n",
    "        \n",
    "        hidden = ( \n",
    "            weigths.new(self.lstm_layers,1,LSTM_HIDDEN_SIZE).to(device)\n",
    "        ,   weigths.new(self.lstm_layers,1,LSTM_HIDDEN_SIZE).to(device)\n",
    "                 )\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "    \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lstm = ConvLSTM(model,lstm_layers=2)\n",
    "conv_lstm.to(device)\n",
    "\n",
    "base_criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/pytorch_challenge/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: accuracy 0.4 loss:0.6974928975105286\n",
      "Epoch 1: accuracy 0.4 loss:0.6968191266059875\n",
      "Epoch 2: accuracy 0.4 loss:0.6959928274154663\n",
      "Epoch 3: accuracy 0.4 loss:0.6955042481422424\n",
      "Epoch 4: accuracy 0.4 loss:0.6950117349624634\n",
      "Epoch 5: accuracy 0.4 loss:0.6945472955703735\n",
      "Epoch 6: accuracy 0.4 loss:0.6940917372703552\n",
      "Epoch 7: accuracy 0.4 loss:0.6936908960342407\n",
      "Epoch 8: accuracy 0.4 loss:0.6934806108474731\n",
      "Epoch 9: accuracy 0.4 loss:0.6931565999984741\n",
      "Epoch 10: accuracy 0.6 loss:0.6923632621765137\n",
      "Epoch 11: accuracy 0.6 loss:0.691911518573761\n",
      "Epoch 12: accuracy 0.6 loss:0.6925774812698364\n",
      "Epoch 13: accuracy 0.6 loss:0.6912993788719177\n",
      "Epoch 14: accuracy 0.6 loss:0.6908305883407593\n",
      "Epoch 15: accuracy 0.6 loss:0.6916059851646423\n",
      "Epoch 16: accuracy 0.6 loss:0.6903756260871887\n",
      "Epoch 17: accuracy 0.6 loss:0.6895414590835571\n",
      "Epoch 18: accuracy 0.6 loss:0.6893054842948914\n",
      "Epoch 19: accuracy 0.6 loss:0.6889806389808655\n",
      "Epoch 20: accuracy 0.6 loss:0.6893985867500305\n",
      "Epoch 21: accuracy 0.6 loss:0.6887177228927612\n",
      "Epoch 22: accuracy 0.6 loss:0.6877778768539429\n",
      "Epoch 23: accuracy 0.6 loss:0.6879743337631226\n",
      "Epoch 24: accuracy 0.6 loss:0.687228798866272\n",
      "Epoch 25: accuracy 0.6 loss:0.6869603991508484\n",
      "Epoch 26: accuracy 0.6 loss:0.6867324113845825\n",
      "Epoch 27: accuracy 0.6 loss:0.6871582865715027\n",
      "Epoch 28: accuracy 0.6 loss:0.68610680103302\n",
      "Epoch 29: accuracy 0.6 loss:0.685870349407196\n",
      "Epoch 30: accuracy 0.6 loss:0.6854605078697205\n",
      "Epoch 31: accuracy 0.6 loss:0.686020016670227\n",
      "Epoch 32: accuracy 0.6 loss:0.6854438781738281\n",
      "Epoch 33: accuracy 0.6 loss:0.6846485733985901\n",
      "Epoch 34: accuracy 0.6 loss:0.6843833923339844\n",
      "Epoch 35: accuracy 0.6 loss:0.684097409248352\n",
      "Epoch 36: accuracy 0.6 loss:0.6841236352920532\n",
      "Epoch 37: accuracy 0.6 loss:0.6836649775505066\n",
      "Epoch 38: accuracy 0.6 loss:0.6836972236633301\n",
      "Epoch 39: accuracy 0.6 loss:0.6836957931518555\n",
      "Epoch 40: accuracy 0.6 loss:0.6833409070968628\n",
      "Epoch 41: accuracy 0.6 loss:0.6829142570495605\n",
      "Epoch 42: accuracy 0.6 loss:0.6827093362808228\n",
      "Epoch 43: accuracy 0.6 loss:0.6829091906547546\n",
      "Epoch 44: accuracy 0.6 loss:0.6822065711021423\n",
      "Epoch 45: accuracy 0.6 loss:0.682073712348938\n",
      "Epoch 46: accuracy 0.6 loss:0.6820638179779053\n",
      "Epoch 47: accuracy 0.6 loss:0.681728184223175\n",
      "Epoch 48: accuracy 0.6 loss:0.681779682636261\n",
      "Epoch 49: accuracy 0.6 loss:0.6813514828681946\n",
      "Epoch 50: accuracy 0.6 loss:0.6813424825668335\n",
      "Epoch 51: accuracy 0.6 loss:0.6816290020942688\n",
      "Epoch 52: accuracy 0.6 loss:0.6809474229812622\n",
      "Epoch 53: accuracy 0.6 loss:0.6809019446372986\n",
      "Epoch 54: accuracy 0.6 loss:0.6806207895278931\n",
      "Epoch 55: accuracy 0.6 loss:0.6805477142333984\n",
      "Epoch 56: accuracy 0.6 loss:0.6806741952896118\n",
      "Epoch 57: accuracy 0.6 loss:0.6802282333374023\n",
      "Epoch 58: accuracy 0.6 loss:0.6800762414932251\n",
      "Epoch 59: accuracy 0.6 loss:0.6803261637687683\n",
      "Epoch 60: accuracy 0.6 loss:0.6798053979873657\n",
      "Epoch 61: accuracy 0.6 loss:0.6795748472213745\n",
      "Epoch 62: accuracy 0.6 loss:0.6796445250511169\n",
      "Epoch 63: accuracy 0.6 loss:0.6793826818466187\n",
      "Epoch 64: accuracy 0.6 loss:0.6794384717941284\n",
      "Epoch 65: accuracy 0.6 loss:0.6792611479759216\n",
      "Epoch 66: accuracy 0.6 loss:0.6791081428527832\n",
      "Epoch 67: accuracy 0.6 loss:0.6790681481361389\n",
      "Epoch 68: accuracy 0.6 loss:0.6788421869277954\n",
      "Epoch 69: accuracy 0.6 loss:0.6790746450424194\n",
      "Epoch 70: accuracy 0.6 loss:0.6787160038948059\n",
      "Epoch 71: accuracy 0.6 loss:0.6786515712738037\n",
      "Epoch 72: accuracy 0.6 loss:0.6785253286361694\n",
      "Epoch 73: accuracy 0.6 loss:0.6784407496452332\n",
      "Epoch 74: accuracy 0.6 loss:0.6782997846603394\n",
      "Epoch 75: accuracy 0.6 loss:0.6784456968307495\n",
      "Epoch 76: accuracy 0.6 loss:0.6780779957771301\n",
      "Epoch 77: accuracy 0.6 loss:0.6779944896697998\n",
      "Epoch 78: accuracy 0.6 loss:0.6777815818786621\n",
      "Epoch 79: accuracy 0.6 loss:0.6777886748313904\n",
      "Epoch 80: accuracy 0.6 loss:0.6777071952819824\n",
      "Epoch 81: accuracy 0.6 loss:0.6775389909744263\n",
      "Epoch 82: accuracy 0.6 loss:0.6773134469985962\n",
      "Epoch 83: accuracy 0.6 loss:0.6773896813392639\n",
      "Epoch 84: accuracy 0.6 loss:0.6773313283920288\n",
      "Epoch 85: accuracy 0.6 loss:0.6772681474685669\n",
      "Epoch 86: accuracy 0.6 loss:0.6770374774932861\n",
      "Epoch 87: accuracy 0.6 loss:0.6768518090248108\n",
      "Epoch 88: accuracy 0.6 loss:0.6767935156822205\n",
      "Epoch 89: accuracy 0.6 loss:0.6770784854888916\n",
      "Epoch 90: accuracy 0.6 loss:0.6767717599868774\n",
      "Epoch 91: accuracy 0.6 loss:0.6768423318862915\n",
      "Epoch 92: accuracy 0.6 loss:0.6764925122261047\n",
      "Epoch 93: accuracy 0.6 loss:0.6765018701553345\n",
      "Epoch 94: accuracy 0.6 loss:0.676421046257019\n",
      "Epoch 95: accuracy 0.6 loss:0.6763899326324463\n",
      "Epoch 96: accuracy 0.6 loss:0.6761080622673035\n",
      "Epoch 97: accuracy 0.6 loss:0.6762158274650574\n",
      "Epoch 98: accuracy 0.6 loss:0.6759928464889526\n",
      "Epoch 99: accuracy 0.6 loss:0.6759865880012512\n",
      "[0.6974929, 0.6968191, 0.6959928, 0.69550425, 0.69501173, 0.6945473, 0.69409174, 0.6936909, 0.6934806, 0.6931566, 0.69236326, 0.6919115, 0.6925775, 0.6912994, 0.6908306, 0.691606, 0.6903756, 0.68954146, 0.6893055, 0.68898064, 0.6893986, 0.6887177, 0.6877779, 0.68797433, 0.6872288, 0.6869604, 0.6867324, 0.6871583, 0.6861068, 0.68587035, 0.6854605, 0.68602, 0.6854439, 0.6846486, 0.6843834, 0.6840974, 0.68412364, 0.683665, 0.6836972, 0.6836958, 0.6833409, 0.68291426, 0.68270934, 0.6829092, 0.6822066, 0.6820737, 0.6820638, 0.6817282, 0.6817797, 0.6813515, 0.6813425, 0.681629, 0.6809474, 0.68090194, 0.6806208, 0.6805477, 0.6806742, 0.68022823, 0.68007624, 0.68032616, 0.6798054, 0.67957485, 0.6796445, 0.6793827, 0.6794385, 0.67926115, 0.67910814, 0.67906815, 0.6788422, 0.67907465, 0.678716, 0.6786516, 0.6785253, 0.67844075, 0.6782998, 0.6784457, 0.678078, 0.6779945, 0.6777816, 0.6777887, 0.6777072, 0.677539, 0.67731345, 0.6773897, 0.6773313, 0.67726815, 0.6770375, 0.6768518, 0.6767935, 0.6770785, 0.67677176, 0.67684233, 0.6764925, 0.67650187, 0.67642105, 0.67638993, 0.67610806, 0.6762158, 0.67599285, 0.6759866]\n",
      "[0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_accuracies  = []\n",
    "total_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    \n",
    "    epoch_predictions = []\n",
    "    epoch_real_values = []\n",
    "    epoch_losses = []\n",
    "    # en cada epoch procesar los pacientes en orden aleatorio\n",
    "    pacientes = np.random.choice(np.array(diagnosticos.paciente),size= len(diagnosticos.paciente),replace=False)\n",
    "    \n",
    "    conteo_pacientes = 0\n",
    "    for paciente in pacientes:\n",
    "        #TODO: remover esta validacion, solo puesta para probar una unica iteracion en compu lenta\n",
    "        if conteo_pacientes >= 99999999:\n",
    "            break\n",
    "        h = conv_lstm.init_hidden()\n",
    "        \n",
    "        conv_lstm.zero_grad()\n",
    "        imagenes_paciente = diccionario_imagenes_pacientes.get(paciente)\n",
    "        diagnostico_hemorragia_paciente = np.array(float(diagnosticos[diagnosticos.paciente==paciente].hemorragia))\n",
    "        tensor_diagnostico_hemorragia_paciente = torch.Tensor(diagnostico_hemorragia_paciente).view((1,1)).to(device)\n",
    "        \n",
    "        indices_imagenes_pacientes = np.arange(0,len(imagenes_paciente)-1,step=1)\n",
    "        indices_aleatorios_imagenes = np.random.choice(indices_imagenes_pacientes,len(indices_imagenes_pacientes),replace=False)\n",
    "        \n",
    "        losses = []\n",
    "        for indice in indices_aleatorios_imagenes:\n",
    "            h = tuple([each.data for each in h])\n",
    "            last_image =  int(indice  == indices_aleatorios_imagenes[-1])\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            \n",
    "            imagen_paciente =  np.expand_dims(imagenes_paciente[indice],2)\n",
    "            imagen_paciente =  np.repeat(imagen_paciente,3,axis=2)\n",
    "               \n",
    "            tensor_imagen_paciente =  train_data_transforms(imagen_paciente).unsqueeze(0).to(device)\n",
    "            \n",
    "            \n",
    "            #print(\"Alimentando paciente {} e imagen {} al modelo\".format(paciente,indice),imagen_paciente.shape)\n",
    "            \n",
    "            diagnostico_hemorragia_aproximado,h  = conv_lstm(tensor_imagen_paciente,h)\n",
    "            \n",
    "            \n",
    "            loss = base_criterion(diagnostico_hemorragia_aproximado,tensor_diagnostico_hemorragia_paciente)\n",
    "            \n",
    "            losses.append(loss.view((1,1)))\n",
    "            \n",
    "            if last_image:\n",
    "                y_hat = diagnostico_hemorragia_aproximado.data.cpu().numpy()[0][0]\n",
    "                y_hat_hard = float(y_hat >= 0.5)\n",
    "                epoch_predictions.append(y_hat_hard)\n",
    "                epoch_real_values.append(float(diagnostico_hemorragia_paciente))\n",
    "                \n",
    "                #print(\"--Flag ultima imagen:{} diagnostico:{} valor real{}\".format(last_image,y_hat,diagnostico_hemorragia_paciente))\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                patient_loss = torch.cat(losses).mean()\n",
    "                \n",
    "                \n",
    "                #patient_loss.backward()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(conv_lstm.lstm.parameters(), 5.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_losses.append(patient_loss.data.cpu().numpy())\n",
    "\n",
    "                \n",
    "        conteo_pacientes += 1\n",
    "        \n",
    "            \n",
    "    epoch_predictions = np.array(epoch_predictions)\n",
    "    epoch_real_values = np.array(epoch_real_values)\n",
    "    correct_predictions = epoch_predictions == epoch_real_values\n",
    "    accuracy = np.average(correct_predictions)\n",
    "    epoch_avg_loss = np.average(epoch_losses)\n",
    "    total_losses.append(epoch_avg_loss)\n",
    "    \n",
    "    total_accuracies.append(accuracy)\n",
    "    print(\"Epoch {}: accuracy {} loss:{}\".format(epoch,accuracy,epoch_avg_loss))\n",
    "\n",
    "#print(np.average(total_accuracies))\n",
    "print(total_losses)\n",
    "print(total_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xl8VNX9//HXJ3sIWQgJCSRAQMK+EzYRt7qAWnEXXNtasYtftVZbtd9++6utbfXbun1dca8bAnWtCyqKUgRKEFD2fQlLCCEkISH7+f0xI40hyAAJN5l5Px+PPMKcOTPzuY+r8845995zzTmHiIhImNcFiIhIy6BAEBERQIEgIiJ+CgQREQEUCCIi4qdAEBERQIEgIiJ+CgQREQEUCCIi4hfhdQFHIiUlxWVlZXldhohIq7Jo0aLdzrnUw/VrVYGQlZVFbm6u12WIiLQqZrY5kH6aMhIREUCBICIifgoEEREBFAgiIuKnQBAREUCBICIifgoEEREBQiQQPli2g5cXBHQarohIyAqJQHhryXb+8t4qisurvS5FRKTFColAuOl72ZRW1vDM3I1elyIi0mKFRCD06ZjAuH7pPDd3I8X7NUoQEWlMSAQC+EcJFTU8p1GCiEijQiYQ+nZK4Ky+aTz7r42UVGiUICLSUMgEAvhGCSUVNTw/d5PXpYiItDghFQj9MxI5o08Hnpu7kYrqWq/LERFpUUIqEAB+cGI3isqr+WDZTq9LERFpUUIuEE48oT1Z7dvwyoItXpciItKihFwghIUZV4zswr837WFNfqnX5YiItBghFwgAlwzrTFR4mEYJIiL1hGQgJMdFMX5AOv/4Mo/9VTq4LCICIRoIAFeO7EppRQ3vfLXd61JERFqEkA2E4Vnt6NGhLS/O20xdnfO6HBERz4VsIJgZN5zcna+3FfOSlsYWEQndQAC4ZFgmp/ZK5U/vrWRDwT6vyxER8VRIB4KZce/FA4mOCOeX05dSU1vndUkiIp4J6UAASEuI4Q8X9Gfxlr08+fkGr8sREfFMyAcCwPmDOnHuwI48+PEaVmwv8bocERFPBBQIZjbOzFab2Tozu+MQfS4zsxVmttzMXqnXfq+ZLfP/XF6v/Xkz22hmS/w/g499c47eHyf0JzE2ilunLaGqRlNHIhJ6DhsIZhYOPAqMB/oCk8ysb4M+2cCdwBjnXD/gFn/7ucBQYDAwErjdzBLqvfR259xg/8+Sptigo9UuLoq/XDSAVTtLeXjWWi9LERHxRCAjhBHAOufcBudcFTAVmNCgz/XAo865IgDn3C5/e1/gM+dcjXOuDFgKjGua0pveGX3TuGRYJo/NXsfiLUVelyMiclwFEggZwNZ6j/P8bfX1BHqa2Vwzm29m33zpLwXGm1kbM0sBTgM613vdPWb2lZk9YGbRR7kNTep/vt+X9IQYfjl9qZa1EJGQEkggWCNtDS/tjQCygVOBScDTZpbknPsQeA/4AngVmAfU+F9zJ9AbGA4kA79u9MPNJptZrpnlFhQUBFDusUmIieS+SwaxoaCMP767otk/T0SkpQgkEPL49l/1mUDDBYDygLecc9XOuY3AanwBgXPuHv8xgjPxhctaf/sO51MJPIdvauogzrkpzrkc51xOamrqkWzbUTspO4XJJ3fn5QVbdCMdEQkZgQTCQiDbzLqZWRQwEXi7QZ838U0H4Z8a6glsMLNwM2vvbx8IDAQ+9D/u6P9twAXAsmPfnKZz21m96J+RwB2vf8WO4v1elyMi0uwOGwjOuRrgRmAmsBKY5pxbbmZ3m9n5/m4zgUIzWwF8iu/soUIgEpjjb58CXOV/P4CXzexr4GsgBfhjU27YsYqKCOPhiUOoqqnjF68toVYL4IlIkDPnWs8XXU5OjsvNzT2unzktdyu/mvEVt53VkxtPzz6uny0i0hTMbJFzLudw/XSl8mFcOiyT8wd14oGP17Jw0x6vyxERaTYKhMMwM+65sD8ZSbHc/Opi9pZXeV2SiEizUCAEID4mkkeuGELBvkp+NeMrWtM0m4hIoBQIARqYmcSvzu7NhyvyeeIzrYoqIsFHgXAEfjy2G+cN7Mh9M1fx0Yp8r8sREWlSCoQjYGb89dJBDMxI5Oapi1m5Q0tli0jwUCAcoZjIcKZck0NCTCQ/fiGXjbvLvC5JRKRJKBCOQlpCDE9fm0NJRTVnP/g5j3yyVvdQEJFWT4FwlPpnJDLr1lM4s08af/1wDec+PIc1+aVelyUictQUCMegQ0IMj145lGeuzWHv/mouf3Iey7YVe12WiMhRUSA0ge/1SWP6DaOJjQzniqfms3TrXq9LEhE5YgqEJpKVEsdrN4wmqU0UVz69QCMFEWl1FAhNqHNyG6bdMJroiDAe/Fj3ZRaR1kWB0MTSE2O4YmQXZq3KZ+uecq/LEREJmAKhGVwxsgthZry0YLPXpYiIBEyB0Aw6JsZydr80Xlu4lYrqWq/LEREJiAKhmVwzOou95dW8vbTh7ad9thSWs6uk4jhXJSJyaAqEZjKyWzK90uJ54YtNBy2Xvb+qlose/4LbZnzlUXUiIgdTIDQTM+OaE7uyfHsJX2759nUJL83fzO59lSzYUKgpJRFpMRQIzeiCwRkkxkbyh3+uoLrWt9ZReVUNT36+nuS4KCpr6vhyS5HHVYqI+CgQmlFcdAR/unAAS7bu5a8zVwPfjA6quP+yQYSHGV+sK/S4ShERHwVCMzt3YEeuHNmFJz/fwHtf7+DJzzYwNjuFU3t1YFBmInPX7/a6RBERQIFwXPz2vL70To/n5698SWFZFbeckQ3AmB4pfJVXTGlFtccViogoEI6LmMhwHrliKLGR4ZzSM5VhXZMBOPGEFGrrHAs27PG4QhERiPC6gFDRo0NbPrr1FNq1iTzQNrRrEjGRYcxdv5sz+qZ5WJ2IiEYIx1VGUixtov6TwdER4QzPSj7kgeX8kgqWaCltETlOFAgeG31Ce1bnl1JQWnnQc7dNX8qVT83X7TlF5LhQIHhszAkpAHzR4GyjtfmlzFm7m7KqWl2rICLHhQLBY/0zEkmIiWDuum8HwvNfbCIqIozwMGPO2gKPqhORUKJA8Fh4mHFWv3TeWLztwF3Wisuref3LbUwY1InBnZP411pdqyAizU+B0AL85pw+JMdFcfPUxeyvqmVa7lb2V9fygzFZjM1O4attxRSVVXldpogEOQVCC9AuLor7LxvMht1l/P6d5bwwbxMjuiXTr1MiY7NTcQ5d0SwizU6B0EKM6ZHC5LHdmbpwK3lF+/nhiVkADMpMJD4mQtNGItLsdGFaC/LLs3rxxfpCSiuqOdN/oVpEeBgnntCeOWt345zDzDyuUkSClQKhBYmKCGPGT0dTUVVHRPh/Bm8nZacyc3k+G3eX0T21rYcVikgw05RRCxMdEU5iveUtAE7O9l2rMEfTRiLSjBQIrUDX9nF0SW6j6xFEpFkFFAhmNs7MVpvZOjO74xB9LjOzFWa23Mxeqdd+r5kt8/9cXq+9m5ktMLO1ZvaamUUd++YEr7HZKcxbX8j+Kt1yU0Sax2EDwczCgUeB8UBfYJKZ9W3QJxu4ExjjnOsH3OJvPxcYCgwGRgK3m1mC/2X3Ag8457KBIuC6JtmiIDVhcAZlVbW889V2r0sRkSAVyAhhBLDOObfBOVcFTAUmNOhzPfCoc64IwDm3y9/eF/jMOVfjnCsDlgLjzHeqzOnADH+/F4ALjm1TgtvwrHb06NCWV/+9xetSRCRIBRIIGcDWeo/z/G319QR6mtlcM5tvZuP87UuB8WbWxsxSgNOAzkB7YK9zruY73lPqMTMmjejC4i17WbmjxOtyRCQIBRIIjZ347ho8jgCygVOBScDTZpbknPsQeA/4AngVmAfUBPievg83m2xmuWaWW1AQ2gdVLx6aQVREGK8s0ChBRJpeIIGQh++v+m9kAg0nsvOAt5xz1c65jcBqfAGBc+4e59xg59yZ+IJgLbAbSDKziO94T/yvn+Kcy3HO5aSmpga6XUEpqU0U5w7oyJuLt1FeVXP4F4iIHIFAAmEhkO0/KygKmAi83aDPm/img/BPDfUENphZuJm197cPBAYCHzrnHPApcIn/9dcCbx3rxoSCK0Z2obSyhn8u3eF1KSISZA4bCP55/huBmcBKYJpzbrmZ3W1m5/u7zQQKzWwFvi/6251zhUAkMMffPgW4qt5xg18Dt5rZOnzHFJ5pyg0LVjldfQeXX16wmbq6RmfZRESOivn+WG8dcnJyXG5urtdleO7lBZv5zRvLGN29PX+7bBCdkmIb7VdVU0dUhK49FAl1ZrbIOZdzuH76tmiFrhjRhfsuHsjSvL2Me/Bz3ll68OGX97/ewaDff8jna0L7QLyIBE6B0AqZGZcN78x7N42le2pb/uvVxTw+e/2B51fvLOWX05eyv7qWv320htY0ChQR7ygQWrGslDim/2Q05w/qxL0frOKhj9dSXF7N5BdziYuO4JYzslm6dS+fa1E8EQmAlr9u5SLDw3jg8sFERYTxwMdrmJa7lV2lFbx6/SgGZiYxbeFWHp61lpOzU3QvBRH5ThohBIHwMOO+iwcyaUQXtu3dz+++34+crGSiIsL46Wk9WLS5iHnrC70uU0RaOJ1lFEScc2zbu5/Mdm0OtFXW1HLKfbPp2r4Nr90w2sPqRMQrOssoBJnZt8IAfDfcueGU7izYuIeFm/Z4VJmItAYKhBAwcXgX2kZHMG3h1sN3FpGQpUAIAbFR4ZzdL50Plu2kolo32BGRxikQQsQFQzpRWlnDJ6t2Hb6ziIQkBUKIOPGEFFLjo3lz8TavSxGRFkqBECLCw4zvD+zE7NUFFJdXe12OiLRACoQQcsGQTlTV1vHeMi2dLSIHUyCEkAEZiXRPjePNxduorXO8vGAzI//0MS/O3+x1aSLSAigQQoiZccHgDBZs3MO5D8/hN28so3h/NY98spaqmjqvyxMRjykQQsyEwZ0IDzNfEFwxhCevziG/pLLRJbRFJLRocbsQ07V9HDNvOZlOSTG0iYrAOUevtHiemrOBi4ZmaAE8kRCmEUII6tGhLW2ifH8LmBnXje3Gqp2lzF2nBfBEQpkCQZgwuBMpbaN5as4Gr0sREQ8pEIToiHCuHd2Vz9YUsCa/1OtyRMQjCgQB4KpRXYmNDOeWqUvIKyo/ZL/cTXv4l+7AJhKUFAgCQLu4KB67aihbi8o5/5G5jd5QZ01+Kdc8+29unrqY2rrWcx8NEQmMAkEOOK1XB976+RiS46K46pkFPPnZ+gNf/CUV1fzkxUVU1dRRWFZFru6tIBJ0FAjyLd1T2/LGz07kzD5p/Pn9VUyaMp9Nu8u4bdpSNu8p56lrcoiKCGPm8nyvSxWRJqZAkIPEx0Ty+FVD+dulg1i5s4Qz7v+MD1fkc9c5fTitdwfG9khh5vKdtKbbr4rI4SkQpFFmxsXDMpl5y8mc3rsDV4/qyo/GZAFwdr90tu3dz/LtJd4WKSJNSlcqy3fqlBTLlGu+fW/uM/qmEfY6fLBsJ/0zEj2qTESamkYIcsSS46IY0S2Zmct3el2KiDQhBYIclXH90lm7ax/rC/Z5XYqINBEFghyVs/qlA2iUIBJEFAhyVDolxTIwM5HpuXls27v/O/vW6SI2kVZBgSBH7Rdn9mRXSQXjHvyct5Zsa7TPjEV5jP7LLLYUHno5DBFpGRQIctRO69WB924eS48Obbl56hJun770W6OBPWVV/OGfK8gvqeSe91Z4WKmIBEKBIMeka/s4pt8wmp+degLTF+Xxt49WH3juvg9WUVZZw6XDMpm5PJ+567QonkhLpkCQYxYRHsbtZ/di4vDOPPrpet5Zup3FW4p4LXcrPxyTxR8u6E+X5Db8/p3l1NTq3s0iLZUCQZqEmXH3hP7kdG3H7TOWctv0paS2jebmM3oSExnOb87tw5r8fby8YIvXpYrIISgQpMlERYTx+FXDSG4TxfqCMn5zbh/aRvsuhj+rbxon9Ujh/o/WUFxe7XGlItKYgALBzMaZ2WozW2dmdxyiz2VmtsLMlpvZK/Xa7/O3rTSzh81/F3czm+1/zyX+nw5Ns0nipdT4aP5+3Uh+f34/zh/U6UC7mXHnOb0p3l/NtNytHlYoIody2LWMzCwceBQ4E8gDFprZ2865FfX6ZAN3AmOcc0XffLmb2YnAGGCgv+u/gFOA2f7HVzrncptoW6SF6NGhLT06tD2ovV+nREZkJfPi/M1cd1I3wsLMg+pE5FACGSGMANY55zY456qAqcCEBn2uBx51zhUBOOd2+dsdEANEAdFAJKCF9EPY1aO7smVPOZ+tKfC6FBFpIJBAyADqj/Hz/G319QR6mtlcM5tvZuMAnHPzgE+BHf6fmc65lfVe95x/uui330wlSXA7u186HeKjeWHeJq9LEZEGAgmExr6oG65FEAFkA6cCk4CnzSzJzHoAfYBMfCFyupmd7H/Nlc65AcBY/8/VjX642WQzyzWz3IIC/VXZ2kVFhHHFyC7MXl3Apt1lXpcjIvUEEgh5QOd6jzOB7Y30ecs5V+2c2wisxhcQFwLznXP7nHP7gPeBUQDOuW3+36XAK/impg7inJvinMtxzuWkpqYGvmXSYl0xogsRYcZL8zd7XYqI1BNIICwEss2sm5lFAROBtxv0eRM4DcDMUvBNIW0AtgCnmFmEmUXiO6C80v84xd8/EjgPWNYUGyQtX4eEGMb1T2da7lZKK3QKqkhLcdhAcM7VADcCM4GVwDTn3HIzu9vMzvd3mwkUmtkKfMcMbnfOFQIzgPXA18BSYKlz7h18B5hnmtlXwBJgG/BU026atGTXndSNfZU1XPrEPDYXaupIpCWw1nSj9JycHJebq7NUg8Xnawq4aepi6uocD00awmm9dCmKSHMws0XOuZzD9dOVyuKZk3um8s6NJ5HRrg0/en4hT362ntb0B4pIsFEgiKc6J7fh9Z+eyDkDOvLn91dx1xtfU60F8EQ8cdgrlUWaW2xUOP83cQhZ7dvw6Kfr2bKnnJOzUykqr6assoarR3elZ1q812WKBD0FgrQIYWHG7Wf3Jqt9HHe98TVz1xUSFR4GBrPX7OLdm8aSEBPpdZkiQU2BIC3KpTmdGT+gIwBxUeF8uaWIy56cz2/eWMbDEwejC9pFmo+OIUiL0zY6grbREZgZw7om84szsnln6XamL8rzujSRoKYRgrR4Pz21B3PXFfK7t5azrWg/e8qqKCitpKq2DgPMfGskXZrT+bDvJSKHpkCQFi88zHjg8sFc+NhcHpq1lsTYSFLjo4mJDMM5KKmo5uOVu6ipc0wa0cXrckVaLQWCtArpiTF8/qvTqHOO6Ijwbz1XVVPH5BdzueuNr4mLjvjWjXlEJHA6hiCtRmR42EFhAP5bd145jOFZydz62hJmrdQtN0SOhgJBgkJsVDjPXJtD304J/Neri1mbX+p1SSKtjgJBgkZ8TCRTrs6hTVQ4N7y4SCupihwhBYIElfTEGB65Yiib95Rz2/SlWhtJ5AgoECTojOrenjvH92bm8nwe+GgNdXXfDoV1u0r5ZJWOM4g0pLOMJChdd1I3lm0r5uFP1vGvdbu558IBZLaL5aGP1/L8F5uoqXO8f/NY+nRM8LpUkRZD90OQoOWcY8aiPP78/iqK91eTGBtJUXkVlw7L5N2vdvC9Pmk8PGmI12WKNLtA74egEYIELTPj0pzOnNEnjf/9cDUbCvZxx/g+DO6cRLs2UTw1ZwO3ntmTrJQ4r0sVaRF0DEGCXru4KP504QCmTh7N4M5JgG9KKSI8jCc/X+9xdSIthwJBQlKHhBguy8lkxqI8dhZXeF2OSIugQJCQdcPJJ1Dn4Kk5G7wuRaRF0DEECVmdk9swYVAnXpy3mf3VtVwxogv9MxK9LkvEMwoECWl3ntMHM+P1L/N4ZcEWenRoS3JcFNERYbSPi+LX43vTMTHW6zJFjguddioCFO+v5q0l2/hk1S72V9VSWVPHmvxSUuOjmTp5lEJBWrVATztVIIgcwpdbirjmmX+T0jaKqZNHk54YA/iW246K0OE3aT0UCCJN4JtQSIyNpGNiDOsL9lFaUcP/fL8v14zO8ro8kYAEGgj6M0fkOwzt0o4XfjSCpDaRhIcZ4/p3ZFjXdtz9zgoWbtrT6Gsqqmv535mrmLtu93GuVuTYaIQgcoRKKqo5///+RVlVLe/+10l0SIg58FzhvkpueHERuZuLyEiKZfbtpxIZrr+7xFsaIYg0k4SYSJ64ehj7Kmq48ZXFbN1Tzq6SCpZvL+aix7/gq23FXDu6K9v27ueNL7d5Xa5IwHTaqchR6J2ewF8uHsDNU5cw9r5PD7S3j4vi1etHMbRLEou2FPHo7HVcNDSDCI0SpBVQIIgcpQmDM0hPiGHznnKqa+uorXN8r08aGUm+U1RvOj2byS8u4u2l27loaKbH1YocngJB5BiM7N6ekd3bN/rcmX3T6J0ezyOfrmPC4AzCw+w4VydyZDSOFWkmZsZN38tmQ0EZD328hkWb91BQWqnbekqLpRGCSDMa1y+dIV2SePiTdTz8yToAcrq247GrhtIhPuYwrxY5vnTaqUgzq66tY3NhOVv2lLFqZykPz1pLcpsonvnBcN3CU44LXaks0kIt21bMj1/IpaSimt+c24exPVLpnBxLVW0ds1buYnruVvZV1vDcD0fQNlqDeDl2CgSRFiy/pILJf89laV4x4DtdtdY59pZXk5YQTUFpJecN7MRDEwdjpoPRcmx0T2WRFiwtIYbXfzaG1TtLWby1iC8378U5x4QhGZzUI4XHZ6/jrx+uYVT39lwxsovX5UqICOgsIzMbZ2arzWydmd1xiD6XmdkKM1tuZq/Ua7/P37bSzB42/587ZjbMzL72v+eBdpFQER5m9O2UwJUju/K3ywZx/+WDOaVnKuFhxs9O7cHY7BT+3zvLWbG95KDX5m7aw69nfMXe8ioPKpdgddhAMLNw4FFgPNAXmGRmfRv0yQbuBMY45/oBt/jbTwTGAAOB/sBw4BT/yx4HJgPZ/p9xTbA9IkEhLMx44PLBtGsTyfV/z+Xtpdupqa2jrs7x2Ox1XD5lPq/lbuUP/1zpdakSRAKZMhoBrHPObQAws6nABGBFvT7XA48654oAnHO7/O0OiAGiAAMigXwz6wgkOOfm+d/z78AFwPvHvEUiQSKlbTRPXDWMX05byk2vLubepFgy2sXy7417OHdgRzrER/Pc3E1cMKQTY7NTvS5XgkAgU0YZwNZ6j/P8bfX1BHqa2Vwzm29m4wD8X/ifAjv8PzOdcyv9r887zHuKhLwhXdrx8a2n8NQ1OWQkxbJiewl/vKA/j0wawq/H9aZ7Shx3vfE15VU1XpcqQSCQEUJjc/sNT02KwDftcyqQCcwxs/5ACtDH3wbwkZmdDOwP4D19H242Gd/UEl266OCahJ6wMOPMvmmc2TcN59yBs45iIsP580UDuHzKfO7/cA3/fV7fw7yTyHcLZISQB3Su9zgT2N5In7ecc9XOuY3AanwBcSEw3zm3zzm3D9+U0Ch//8zDvCcAzrkpzrkc51xOaqqGxRLaGp57MdJ/FtKzczfy4vzNWhZDjkkggbAQyDazbmYWBUwE3m7Q503gNAAzS8E3hbQB2AKcYmYRZhaJ74DySufcDqDUzEb5zy66BnirSbZIJMTcOb43J2Wn8ts3l3HtcwvZWVzB/qpaFm7aw2sLt7CrtMLrEqWVOOyUkXOuxsxuBGYC4cCzzrnlZnY3kOuce9v/3FlmtgKoBW53zhWa2QzgdOBrfFNCHzjn3vG/9U+B54FYfCMHHVAWOQrxMZG88MPhvLRgC396dyWn/vVTqmsdtXW+0UJqfDSPXTmU4VnJHlcqLZ2uVBYJIht3lzHl8/Wkto1mYGYSCbGR/GrGUvKK9nPXOX0Y0S2ZtbtK2VhQxpAu7Ti1V6quhA4BWrpCRAAo3l/NL6ct5eOV+Qc9NzyrHb8a11ujhyCnQBCRA+rqHO8v2wlAz7S2ZLZrwz++zOOhWWspKK1kbHYKN5x8AmN6tNeIIQgpEETksPZX1fLCvE0886+NFJRW0rdjAucN6khGUiyZ7WLplZ5w0IqrFdW1lFRU634OrYgCQUQCVllTy5uLt/H0nI2s3bXvQHv7uCjuuXAA4/qnA741lH4xbQmF+6p46ccjGdqlnVclyxFQIIjIUSmrrGH73v1sLiznwVlrWLathAsGdyI9MZYpn68no10shlG8v5rXbhhF73Td5KelUyCIyDGrrq3jsU/X83+frKWmzjFxeGf++7y+FJVVcckTX1DnYMZPRtO1fZzXpcp3UCCISJNZvbOUPWVVjD6h/YG2tfmlXPbkPOqc72ylvp0SGdw5kZN6pBIVEdDK+nKc6AY5ItJkeqXHH9SWnRbPK9eP4snP1rN8ewmfrNpFnYPkuCgmDO7ExUMz6dcpQWcttSIaIYhIk9hfVcv8DYVMX7SVj1fsoqq2jvZxUYzq3p4Te7TnvAGdSGwTeaB/cXk1n6zO5/TeaSTGRn7HO8ux0pSRiHimqKyKj1bmM399IfM2FLKjuIKYyDAuHJLBOQM68uHyfGYsymN/dS3DurbjpetGEhsV7nXZQUuBICItgnOO5dtLeGn+Zt5cso2K6jqiwsM4f3An+nRM4I/vruD0Xh144uphRIbr2ENzUCCISIuzt7yKL9YXMjwrmdT4aABemr+Z/35zGZcMy+R/LxmoYw7NQAeVRaTFSWoTxTkDOn6r7apRXdm9r5IHP17LF+t2M6p7e0Z1b8/Z/dN1bOE40whBRDznnOP1L7cxa1U+8zfsYU9ZFanx0fz+/H6M75+uUcMx0pSRiLRKzjm+3LKX/3lrGcu3l3BGnzS+P6gj1bWOmto6OibFMqxru4PWWJJDUyCISKtWU1vHs3M3cv9Ha6iorvvWc2EG/TMSOatvGteemEV8jKaWvosCQUSCQlFZFXvKq4gMCyM83NhYUMa/N/pOZ124qYjE2Eh+fFI3fjBGwXAoCgQRCXpf5xXz4MdrmLVqFx3io/nLxQM4vXea12W1OAoEEQkZi7cUccc/vmZ1fimXDsvkx2O7s2JHMbmbiigorSQ7rS290hPonR5P95Q4IkLsegcFgoiElMqaWh6etZbHZ6+nzv+11jZCCMkTAAAIs0lEQVQ6gg4J0WwpLKfG3xgdEUav9HiGdmnHjaf3IKVttIdVHx8KBBEJScu2FbNsWzGDOifRMy2e8DCjsqaWDQVlrNpZwortJazYUcLCjUUkxEbw54sGcmbf4J5mUiCIiHyH1TtL+cVrS1ixo4TLcjK5Y3wfkuOiGu27amcJv31zGROHd+HiYZnHudJjpyuVRUS+Q6/0eN78+RgemrWGx2ev5/1lO7nxtB5ce2IWMZH/WWhv9upd3PjKYsqrali4qYjCskomn3yCh5U3H40QRCTkrckv5c/vreTT1QV0TIxh9Ant6ZUWT1VNHQ98vIbe6Qk8cdUw7v1gFe9+vYMbTu7OmX3TWLJ1L6t2lnLOgPQWfXaTpoxERI7Q3HW7eWrOBlbuKCG/pBKAM/p04KGJQ4iLjqC2zvG7t5fx0vwtB17TJiqcqpo6nrhqGGe00GMRCgQRkWNQVFbFzpKKAwemv+Gc46MV+ZgZgzITiY0K56qnF7ByRynP/mA4J2WneFh14xQIIiLHyd7yKiZOmc+mwjJ+NKYbdc53GmxaQgzDs5IZkJHo6X2mdVBZROQ4SWoTxYvXjeTaZ//NY7PXExUeRmS4UVZVC0BMZBg90+JJbRtNanw0w7OSuWhoRotbxVWBICLSBFLjo3n3ppMADnzR795XycKNe1iwcQ8bd5exo7iCJVv3MnXhVnI37+HuCf1b1F3iFAgiIk2k4V/8KW2jGT+gI+Pr3RSors7xt49W8+in69lcWM7vz+/HvA2FfLBsJ3vKqrjrnD6c3DP1eJcO6BiCiIgn/rEojzte/4rqWt938AmpcdQ52Li7jEkjOnPXOX2abPVWHUMQEWnBLh6WSY8ObVm4aQ+n9EwlOy2eiupaHvhoDU/N2cA/l+4gsU0kEWFGRHgYz1ybQ9f2cc1akwJBRMQjgzonMahz0oHHMZHh3HlOH87un8703Dwqa2qpqXXU1NURHRH+He/UNBQIIiItzNAu7Rjapd1x/9yWc3hbREQ8pUAQERFAgSAiIn4BBYKZjTOz1Wa2zszuOESfy8xshZktN7NX/G2nmdmSej8VZnaB/7nnzWxjvecGN91miYjIkTrsQWUzCwceBc4E8oCFZva2c25FvT7ZwJ3AGOdckZl1AHDOfQoM9vdJBtYBH9Z7+9udczOaamNEROToBTJCGAGsc85tcM5VAVOBCQ36XA886pwrAnDO7WrkfS4B3nfOlR9LwSIi0jwCCYQMYGu9x3n+tvp6Aj3NbK6ZzTezcY28z0Tg1QZt95jZV2b2gJkF/52uRURasEACobHl+BqudxEBZAOnApOAp83swNUWZtYRGADMrPeaO4HewHAgGfh1ox9uNtnMcs0st6CgIIByRUTkaARyYVoe0Lne40xgeyN95jvnqoGNZrYaX0As9D9/GfCG/3kAnHM7/P+sNLPngNsa+3Dn3BRgCoCZFZjZ5gBqbkwKsPsoX9uaheJ2h+I2Q2hut7Y5MF0D6RRIICwEss2sG7AN39TPFQ36vIlvZPC8maXgm0LaUO/5SfhGBAeYWUfn3A7zLQ94AbDscIU45456CUAzyw1kcadgE4rbHYrbDKG53drmpnXYQHDO1ZjZjfime8KBZ51zy83sbiDXOfe2/7mzzGwFUIvv7KFCf/FZ+EYYnzV465fNLBXflNQS4CdNs0kiInI0AlrLyDn3HvBeg7b/qfdvB9zq/2n42k0cfBAa59zpR1iriIg0o1C6UnmK1wV4JBS3OxS3GUJzu7XNTahV3SBHRESaTyiNEERE5DuERCAEshZTa2dmnc3sUzNb6V9P6mZ/e7KZfWRma/2/j/8i683MzMLNbLGZ/dP/uJuZLfBv82tmFuV1jU3NzJLMbIaZrfLv89HBvq/N7Bf+/7aXmdmrZhYTjPvazJ41s11mtqxeW6P71nwe9n+3fWVmQ4/ls4M+EOqtxTQe6AtMMrO+3lbVLGqAXzrn+gCjgJ/7t/MOYJZzLhuY5X8cbG4GVtZ7fC/wgH+bi4DrPKmqeT0EfOCc6w0Mwrf9QbuvzSwDuAnIcc71x3fG40SCc18/DzRc7eFQ+3Y8vmu+soHJwOPH8sFBHwgEthZTq+ec2+Gc+9L/71J8XxAZ+Lb1BX+3F/Bd8xE0zCwTOBd42v/YgNOBbxZNDMZtTgBOBp4BcM5VOef2EuT7Gt9ZkbFmFgG0AXYQhPvaOfc5sKdB86H27QTg785nPpDkXxniqIRCIASyFlNQ8V/7MQRYAKR9c1W4/3cH7yprFg8CvwLq/I/bA3udczX+x8G4v7sDBcBz/qmyp80sjiDe1865bcBfgS34gqAYWETw7+tvHGrfNun3WygEQiBrMQUNM2sL/AO4xTlX4nU9zcnMzgN2OecW1W9upGuw7e8IYCjwuHNuCFBGEE0PNcY/Zz4B6AZ0AuLwTZc0FGz7+nCa9L/3UAiEQNZiCgpmFokvDF52zr3ub87/Zgjp/93Y0uSt1RjgfDPbhG8q8HR8I4Yk/7QCBOf+zgPynHML/I9n4AuIYN7XZwAbnXMF/jXRXgdOJPj39TcOtW+b9PstFALhwFpM/jMQJgJve1xTk/PPnT8DrHTO3V/vqbeBa/3/vhZ463jX1lycc3c65zKdc1n49usnzrkrgU/x3X8DgmybAZxzO4GtZtbL3/Q9YAVBvK/xTRWNMrM2/v/Wv9nmoN7X9Rxq374NXOM/22gUUFxv4dAjFhIXppnZOfj+cvxmLaZ7PC6pyZnZScAc4Gv+M59+F77jCNOALvj+p7rUOdfwgFWrZ2anArc5584zs+74RgzJwGLgKudcpZf1NTXz3XL2aSAK30KSP8T3B17Q7msz+z1wOb4z6hYDP8Y3Xx5U+9rMXsV3K4EUIB/4Hb4FRA/at/5wfATfWUnlwA+dc7lH/dmhEAgiInJ4oTBlJCIiAVAgiIgIoEAQERE/BYKIiAAKBBER8VMgiIgIoEAQERE/BYKIiADw/wEOBsPKF+7dWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "* Normalizar  el allocation weighitng con sofmax(en la primera iteración asigna todo el peso a la primera posición de memoria)\n",
    "* Usar arquitectura similar a dueling network o inception para tener 2 caminos en las entradas.\n",
    "* Cambiar el modelo original para leer antes que escribir y usar lo leido para sacar una predicción en ese punto en el tiempo(el modelo original lee de la memoria despues de escribir y usa la info leida en el siguiente paso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
